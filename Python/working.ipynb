{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3509021383.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    install pyarrow\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import structural_analysis\n",
    "import statistics\n",
    "#import structure_validation\n",
    "import importlib\n",
    "from scipy import stats\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path  \n",
    "import vcf\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_allsp = pd.read_pickle(r'C:\\Users\\Adam Martin\\OneDrive - University of Dundee\\Honours Project\\Jalview and Chimera using the alignment from PF00026 first from linux\\Uniprot_Sequences_For_Alignment_To_Allow_VarAlign_TO\\Uniprot_Annotated_Alignment127_PFAM.sto_prointvar_structure_table.p.gz')\n",
    "import pyarrow\n",
    "df_allsp = pd.read_pickle('/homes/2414054/Honours/231006a-AM-honours-project-aps/varalign-data-PF00026/pickles/PF00026.22_swissprot.sto_prointvar_structure_table.p.gz') # GETS CONSENSUS COLUMNS FROM MSA (OCCUPANCY > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATOM_A</th>\n",
       "      <th>ATOM_B</th>\n",
       "      <th>Alignment_column_A</th>\n",
       "      <th>Alignment_column_B</th>\n",
       "      <th>B_iso_or_equiv_A</th>\n",
       "      <th>B_iso_or_equiv_B</th>\n",
       "      <th>CATH_dbAccessionId_A</th>\n",
       "      <th>CATH_dbAccessionId_B</th>\n",
       "      <th>CATH_regionEnd_A</th>\n",
       "      <th>CATH_regionEnd_B</th>\n",
       "      <th>...</th>\n",
       "      <th>pdbx_formal_charge_B</th>\n",
       "      <th>type_symbol_A</th>\n",
       "      <th>type_symbol_B</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>protein_topology</th>\n",
       "      <th>pfam_domain_topology</th>\n",
       "      <th>cath_domain_topology</th>\n",
       "      <th>cath_hierarchical_topology</th>\n",
       "      <th>scop_domain_topology</th>\n",
       "      <th>polymer_topology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>230.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>16.675000</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Protein-Protein</td>\n",
       "      <td>Homoprotein</td>\n",
       "      <td>Homodomain (Self)</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>level_4</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>Intrapolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>177.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>34.367500</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Protein-Protein</td>\n",
       "      <td>Homoprotein</td>\n",
       "      <td>Homodomain (Self)</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>level_4</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>Intrapolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>230.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>16.675000</td>\n",
       "      <td>18.305714</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Protein-Protein</td>\n",
       "      <td>Homoprotein</td>\n",
       "      <td>Homodomain (Self)</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>level_4</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>Intrapolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>300.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>47.157500</td>\n",
       "      <td>18.305714</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Protein-Protein</td>\n",
       "      <td>Homoprotein</td>\n",
       "      <td>Homodomain (Self)</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>level_4</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>Intrapolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NE2</td>\n",
       "      <td>O</td>\n",
       "      <td>229.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>16.737778</td>\n",
       "      <td>18.305714</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Protein-Protein</td>\n",
       "      <td>Homoprotein</td>\n",
       "      <td>Homodomain (Self)</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>level_4</td>\n",
       "      <td>Homodomain</td>\n",
       "      <td>Intrapolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222791</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>424.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.920000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Domain-NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interpolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222792</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Domain-NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interpolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222793</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.217778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Domain-NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interpolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222794</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.244286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Domain-NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interpolymer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222795</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.510000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40.70.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Domain-NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interpolymer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2222796 rows Ã— 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATOM_A ATOM_B  Alignment_column_A  Alignment_column_B  \\\n",
       "0            C      N               230.0               232.0   \n",
       "1            O      N               177.0               232.0   \n",
       "2            O      O               230.0               311.0   \n",
       "3            O      N               300.0               311.0   \n",
       "6          NE2      O               229.0               311.0   \n",
       "...        ...    ...                 ...                 ...   \n",
       "2222791    NaN    NaN               424.0                 NaN   \n",
       "2222792    NaN    NaN               120.0                 NaN   \n",
       "2222793    NaN    NaN               144.0                 NaN   \n",
       "2222794    NaN    NaN               227.0                 NaN   \n",
       "2222795    NaN    NaN               331.0                 NaN   \n",
       "\n",
       "         B_iso_or_equiv_A  B_iso_or_equiv_B CATH_dbAccessionId_A  \\\n",
       "0               16.675000         12.560000           2.40.70.10   \n",
       "1               34.367500         12.560000           2.40.70.10   \n",
       "2               16.675000         18.305714           2.40.70.10   \n",
       "3               47.157500         18.305714           2.40.70.10   \n",
       "6               16.737778         18.305714           2.40.70.10   \n",
       "...                   ...               ...                  ...   \n",
       "2222791         41.920000               NaN           2.40.70.10   \n",
       "2222792         62.100000               NaN           2.40.70.10   \n",
       "2222793         56.217778               NaN           2.40.70.10   \n",
       "2222794         43.244286               NaN           2.40.70.10   \n",
       "2222795         43.510000               NaN           2.40.70.10   \n",
       "\n",
       "        CATH_dbAccessionId_B  CATH_regionEnd_A  CATH_regionEnd_B  ...  \\\n",
       "0                 2.40.70.10             214.0             214.0  ...   \n",
       "1                 2.40.70.10             214.0             214.0  ...   \n",
       "2                 2.40.70.10             214.0             214.0  ...   \n",
       "3                 2.40.70.10             214.0             214.0  ...   \n",
       "6                 2.40.70.10             214.0             214.0  ...   \n",
       "...                      ...               ...               ...  ...   \n",
       "2222791                  NaN             383.0               NaN  ...   \n",
       "2222792                  NaN             147.0               NaN  ...   \n",
       "2222793                  NaN             147.0               NaN  ...   \n",
       "2222794                  NaN             147.0               NaN  ...   \n",
       "2222795                  NaN             147.0               NaN  ...   \n",
       "\n",
       "        pdbx_formal_charge_B type_symbol_A  type_symbol_B  interaction_type  \\\n",
       "0                          ?             N              N   Protein-Protein   \n",
       "1                          ?             N              N   Protein-Protein   \n",
       "2                          ?             N              N   Protein-Protein   \n",
       "3                          ?             N              N   Protein-Protein   \n",
       "6                          ?             N              N   Protein-Protein   \n",
       "...                      ...           ...            ...               ...   \n",
       "2222791                  NaN             N            NaN               NaN   \n",
       "2222792                  NaN             N            NaN               NaN   \n",
       "2222793                  NaN             N            NaN               NaN   \n",
       "2222794                  NaN             N            NaN               NaN   \n",
       "2222795                  NaN             N            NaN               NaN   \n",
       "\n",
       "         protein_topology  pfam_domain_topology cath_domain_topology  \\\n",
       "0             Homoprotein     Homodomain (Self)           Homodomain   \n",
       "1             Homoprotein     Homodomain (Self)           Homodomain   \n",
       "2             Homoprotein     Homodomain (Self)           Homodomain   \n",
       "3             Homoprotein     Homodomain (Self)           Homodomain   \n",
       "6             Homoprotein     Homodomain (Self)           Homodomain   \n",
       "...                   ...                   ...                  ...   \n",
       "2222791               NaN                   NaN           Domain-NaN   \n",
       "2222792               NaN                   NaN           Domain-NaN   \n",
       "2222793               NaN                   NaN           Domain-NaN   \n",
       "2222794               NaN                   NaN           Domain-NaN   \n",
       "2222795               NaN                   NaN           Domain-NaN   \n",
       "\n",
       "        cath_hierarchical_topology  scop_domain_topology  polymer_topology  \n",
       "0                          level_4            Homodomain      Intrapolymer  \n",
       "1                          level_4            Homodomain      Intrapolymer  \n",
       "2                          level_4            Homodomain      Intrapolymer  \n",
       "3                          level_4            Homodomain      Intrapolymer  \n",
       "6                          level_4            Homodomain      Intrapolymer  \n",
       "...                            ...                   ...               ...  \n",
       "2222791                        NaN                   NaN      Interpolymer  \n",
       "2222792                        NaN                   NaN      Interpolymer  \n",
       "2222793                        NaN                   NaN      Interpolymer  \n",
       "2222794                        NaN                   NaN      Interpolymer  \n",
       "2222795                        NaN                   NaN      Interpolymer  \n",
       "\n",
       "[2222796 rows x 166 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_allsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/ANK_analysis/lib/python3.11/site-packages/pandas/compat/_optional.py:132\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ANK_analysis/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_allsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/homes/2414054/Honours/231006a-AM-honours-project-aps/varalign-data-PF00026/pickles/gz.feather\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ANK_analysis/lib/python3.11/site-packages/pandas/core/frame.py:2797\u001b[0m, in \u001b[0;36mDataFrame.to_feather\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary Feather format.\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;124;03m>>> df.to_feather(\"file.feather\")  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeather_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_feather\n\u001b[0;32m-> 2797\u001b[0m \u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ANK_analysis/lib/python3.11/site-packages/pandas/io/feather_format.py:60\u001b[0m, in \u001b[0;36mto_feather\u001b[0;34m(df, path, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_feather\u001b[39m(\n\u001b[1;32m     40\u001b[0m     df: DataFrame,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     44\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    Write a DataFrame to the binary Feather format.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feather\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, DataFrame):\n",
      "File \u001b[0;32m~/miniforge3/envs/ANK_analysis/lib/python3.11/site-packages/pandas/compat/_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "df_allsp.to_feather(\"/homes/2414054/Honours/231006a-AM-honours-project-aps/varalign-data-PF00026/pickles/gz.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struc_info(df):\n",
    "    n_res = len(df.drop_duplicates(['UniProt_dbAccessionId_A', 'PDB_dbAccessionId_A', 'PDB_dbChainId_A', 'PDB_dbResNum_A']))\n",
    "    n_res_un = len(df.drop_duplicates([\"SOURCE_ID_A\", \"Alignment_column_A\"]))\n",
    "    n_strucs =  len(df.PDB_dbAccessionId_A.unique().tolist())\n",
    "    n_prots = len(df.UniProt_dbAccessionId_A.unique().tolist())\n",
    "    n_reps = len(df.SOURCE_ID_A.unique().tolist())\n",
    "    print(\"The dataframe contains information of:\\n{} residues, {} of which are unique\\n{} PDB structures\\n{} different proteins\\n{} unique repeats\".format(n_res, n_res_un, n_strucs, n_prots, n_reps))\n",
    "    \n",
    "get_struc_info(df_allsp) # UNFILTERED STRUCTURAL DATASET STATS\n",
    "\n",
    "\n",
    "############## Need to look into why it only 7 proteins. ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allsp.UniProt_dbAccessionId_B.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allsp.UniProt_dbAccessionId_A.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_allsp.columns.to_numpy())#.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allsp_rsrz_filt = df_allsp\n",
    "df_allsp_rsrz_filt.SS_A = df_allsp_rsrz_filt.SS_A.fillna(\"\") # FILL NA WITH \"\" SO WE CAN PROCESS THE SECONDARY STRUCTURE DATA. NA ARE FOUND ON THOSE RESIDUES PRESENTING NO SS, i.e., COIL\n",
    "print(df_allsp_rsrz_filt.SS_A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_struc_info(df_allsp_rsrz_filt) # FILTERED STRUCTURAL DATASET STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio.SeqIO\n",
    "import Bio.AlignIO\n",
    "from Bio.PDB import *\n",
    "def get_n_seq(seq_file, seq_format = \"fasta\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    seqs = Bio.SeqIO.parse(seq_file,seq_format)\n",
    "    n = 0\n",
    "    for seq in seqs:\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "def get_occ_cols(aln_in, fmt_in): #returns a dictionary of the occupoancy of every column in the alignment\n",
    "    aln = Bio.SeqIO.parse(aln_in, fmt_in)\n",
    "    occ = {}\n",
    "    for rec in aln:\n",
    "        seq = str(rec.seq)\n",
    "        for i in range(0, len(seq)):\n",
    "            if i + 1 not in occ:\n",
    "                occ[i+1] = 0\n",
    "            if seq[i] != '-':\n",
    "                occ[i+1] += 1\n",
    "    return occ\n",
    "\n",
    "def get_cons_cols(aln_in, fmt_in, t = 0.50): #returns the columns with a relative occupancy greater than a threshold\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_seq = get_n_seq(aln_in, fmt_in)\n",
    "    print(n_seq)\n",
    "    aln_occ = get_occ_cols(aln_in, fmt_in)\n",
    "    cons_cols = [col for col, occ in aln_occ.items() if occ/n_seq > t]\n",
    "    return cons_cols\n",
    "cons_cols_allsp = get_cons_cols('/homes/2414054/varalign_folder/Altered_Alignment/Changing_Alignment/Alignment127_PFAM.sto', 'stockholm') # GETS CONSENSUS COLUMNS FROM MSA (OCCUPANCY > 0.5)\n",
    "#cons_cols_allsp = get_cons_cols(r'C:\\Users\\Adam Martin\\OneDrive - University of Dundee\\Honours Project\\Jalview and Chimera using the alignment from PF00026 first from linux\\Uniprot_Sequences_For_Alignment_To_Allow_VarAlign_TO\\Uniprot_Annotated_Alignment127_PFAM.sto', 'stockholm') # GETS CONSENSUS COLUMNS FROM MSA (OCCUPANCY > 0.5)\n",
    "print(len(cons_cols_allsp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsa_class_consensus(df, aln_cols):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_unique = df.drop_duplicates(['UniProt_dbAccessionId_A', 'PDB_dbAccessionId_A', 'PDB_dbChainId_A', 'PDB_dbResNum_A'])\n",
    "    df_unique = df_unique.dropna(subset = [\"RSA_CLASS_A\", \"RSA_A\"])             #\"[RSA_CLASS_UNB_A\", \"RSA_UNB_A\"]   changed to      \"[RSA_CLASS_A\", \"RSA_A\"]\n",
    "    df_unique = df_unique.query('RSA_A != \"\"')\n",
    "    df_unique.Alignment_column_A = df_unique.Alignment_column_A.astype(float).astype(int)\n",
    "    df_cons_cols = df_unique[df_unique.Alignment_column_A.isin(aln_cols)]\n",
    "    get_struc_info(df_cons_cols)\n",
    "    grouped_col_rep = df_cons_cols.groupby([\"Alignment_column_A\", \"SOURCE_ID_A\"])\n",
    "    rsa_class_dict = {}\n",
    "    rsa_dict = {}\n",
    "    i = 1\n",
    "    for k, v in grouped_col_rep:\n",
    "        col = k[0]\n",
    "        if col not in rsa_class_dict:\n",
    "            print(\"Processing consensus column {}\".format(i))\n",
    "            i += 1\n",
    "            rsa_class_dict[col] = []\n",
    "        if col not in rsa_dict:\n",
    "            rsa_dict[col] = []\n",
    "        \n",
    "        #holder = []                                     #\n",
    "        #for x in v.RSA_A.tolist():                      #Changed the RSA_A so that gaps were filled with zeros not sure if this is correct will check later\n",
    "        #    if x ==\"\":                                  #\n",
    "        #        x = 0                                   #\n",
    "        #    holder.append(x)                            #\n",
    "\n",
    "        rsa_list = list(map(float, v.RSA_A.tolist()))             #v.RSA_A.tolist()))           #RSA_UNB_A\n",
    "        #print(rsa_list)\n",
    "        rsa_class_list = v.RSA_CLASS_A.tolist()                 #RSA_CLASS_UNB_A\n",
    "        try:\n",
    "            rsa_class_cons = statistics.mode(rsa_class_list)\n",
    "        except:\n",
    "            rsa_class_cons = rsa_class_list[0]\n",
    "        rsa_median = statistics.median(rsa_list)\n",
    "        rsa_class_dict[col].append(rsa_class_cons)\n",
    "        rsa_dict[col].append(rsa_median)\n",
    "    return rsa_class_dict, rsa_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allsp.query('Alignment_column_A==565').drop_duplicates([\"PDB_dbAccessionId_A\",\"PDB_dbChainId_A\"]).RSA_A.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rsa_class_consensus(df, aln_cols):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_unique = df.drop_duplicates(['UniProt_dbAccessionId_A', 'PDB_dbAccessionId_A', 'PDB_dbChainId_A', 'PDB_dbResNum_A'])\n",
    "    df_unique = df_unique.dropna(subset = [\"RSA_CLASS_A\", \"RSA_A\"])\n",
    "    df_unique = df_unique.query('RSA_A != \"\"')\n",
    "    df_unique.Alignment_column_A = df_unique.Alignment_column_A.astype(float).astype(int)\n",
    "    df_cons_cols = df_unique[df_unique.Alignment_column_A.isin(aln_cols)]\n",
    "    missing = []\n",
    "    for x in aln_cols:\n",
    "        if x in df_cons_cols['Alignment_column_A'].values:\n",
    "            t = 0\n",
    "        else: \n",
    "            missing.append(x)\n",
    "    \n",
    "    grouped_col_rep = df_cons_cols.groupby([\"Alignment_column_A\", \"SOURCE_ID_A\"])\n",
    "    \n",
    "    rsa_class_dict = {}\n",
    "    rsa_dict = {}\n",
    "    i = 1\n",
    "    for k, v in grouped_col_rep:\n",
    "        col = k[0]\n",
    "        if col not in rsa_class_dict:\n",
    "            i += 1\n",
    "            rsa_class_dict[col] = []\n",
    "        if col not in rsa_dict:\n",
    "            rsa_dict[col] = []\n",
    "        for x in missing:\n",
    "            rsa_dict[x] = 0\n",
    "            rsa_class_dict[x] = []\n",
    "    \n",
    "        \n",
    "\n",
    "        rsa_list = v.RSA_A.tolist()\n",
    "        rsa_class_list = v.RSA_CLASS_A.tolist()\n",
    "        try:\n",
    "            rsa_class_cons = statistics.mode(rsa_class_list)\n",
    "        except:\n",
    "            rsa_class_cons = rsa_class_list[0]\n",
    "        rsa_median = statistics.median(rsa_list)\n",
    "        rsa_class_dict[col].append(rsa_class_cons)\n",
    "        rsa_dict[col].append(rsa_median)\n",
    "\n",
    "    rsa_dict = dict(sorted(rsa_dict.items()))\n",
    "    \n",
    "    rsa_class_dict = dict(sorted(rsa_class_dict.items()))\n",
    "    \n",
    "    \n",
    "    return rsa_class_dict, rsa_dict\n",
    "\n",
    "\n",
    "\n",
    "                                                                    #######  rsa_list = list(map(float, v.RSA_A.tolist()))       #RSA_UNB_A\n",
    "\n",
    "rsa_class_dict, rsa_dict = get_rsa_class_consensus(df_allsp_rsrz_filt, cons_cols_allsp) # RETRIEVES RSA DATA FROM TABLE\n",
    "print(len(rsa_class_dict))\n",
    "print(rsa_class_dict)\n",
    "\n",
    "print(len(rsa_dict))\n",
    "print(rsa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsa_class_df(rsa_class_dict, cons_cols_allsp):\n",
    "    n = len(rsa_class_dict.keys())\n",
    "    core = []\n",
    "    surf = []\n",
    "    part = []\n",
    "    for v in rsa_class_dict.values():\n",
    "        core.append(v.count(\"Core\"))\n",
    "        surf.append(v.count(\"Surface\"))\n",
    "        part.append(v.count(\"Part. Exposed\")) \n",
    "    df_dssp = pd.DataFrame(list(zip(core, surf, part)), columns =  [\"core\", \"surf\", \"part\"])\n",
    "    df_dssp[\"tot\"] = df_dssp.core + df_dssp.surf + df_dssp.part\n",
    "    df_dssp[\"p_core\"] = df_dssp.core / df_dssp.tot\n",
    "    df_dssp[\"p_surf\"] = df_dssp.surf / df_dssp.tot\n",
    "    df_dssp[\"p_part\"] = df_dssp.part / df_dssp.tot\n",
    "    df_dssp[\"p_tot\"] = df_dssp.p_core + df_dssp.p_surf + df_dssp.p_part\n",
    "    df_dssp.index = range(1, n + 1)         #cons_cols_allsp\n",
    "    return df_dssp\n",
    "\n",
    "\n",
    "rsa_class_df = get_rsa_class_df(rsa_class_dict, cons_cols_allsp) # CALCULATES PROPORTION OF RSA CLASS PER POSITION\n",
    "rsa_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rsa_consensus(df_dssp, palette = [\"purple\", \"orange\", \"royalblue\"], bwidth = 1, out = None):                                # bwidth = 1\n",
    "    core = list(df_dssp.p_core)\n",
    "    surf = list(df_dssp.p_surf)\n",
    "    part = list(df_dssp.p_part)\n",
    "    n_cols = len(df_dssp)\n",
    "    \n",
    "    print(\"n_cols\", n_cols)\n",
    "    cols = list(df_dssp.index)\n",
    "                 #Added\n",
    "\n",
    "\n",
    "    \n",
    "    bottom = []\n",
    "    for i in range(0, len(core)):\n",
    "        bottom.append(core[i] + part[i])\n",
    "\n",
    "    r = (list(np.arange(0, n_cols, 1)))  \n",
    "    plt.figure(figsize=(180,80))\n",
    "    plt.rcParams.update({\"axes.linewidth\": 10})\n",
    "    plt.bar(r, core,  color=palette[0], edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Core')\n",
    "    plt.bar(r, part, color=palette[1], bottom = core, edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Part')\n",
    "    plt.bar(r, surf,  bottom = bottom, color=palette[2], edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Surf')\n",
    "    plt.xlabel('Occupancy residue position', labelpad = 50, fontsize = 120)\n",
    "    plt.ylabel('p', labelpad = 50, fontsize = 120)\n",
    "    legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize = 120)\n",
    "    legend.get_frame().set_linewidth(10)\n",
    "    legend.get_frame().set_edgecolor(\"black\")\n",
    "    for legobj in legend.legendHandles:\n",
    "        legobj.set_linewidth(7.5)\n",
    "    \n",
    "    plt.tick_params(axis= 'both' , which = 'major', pad = 30, width = 15, length = 50, labelsize = 80)\n",
    "    \n",
    "    plt.xticks(np.arange(0, n_cols, 20))     \n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.axhline(y=0.5, linewidth = 5, linestyle = \"--\", color = 'black')\n",
    "    plt.xlim(cols[0], cols[-1])                                                           \n",
    "    plt.ylim(0, 1.025)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "plot_rsa_consensus(rsa_class_df, palette = [\"purple\", \"orange\", \"royalblue\"], bwidth = 1)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def medianvs(v):\n",
    "    if v == 0:\n",
    "        vs= 0\n",
    "    else:\n",
    "        vs = (statistics.median(v))                                     #vs = statistics.median(v) for v in rsa_dict.values()]  #median\n",
    "    return vs\n",
    "\n",
    "def medianCI(data, ci, p):\n",
    "    '''\n",
    "    data: pandas datafame/series or numpy array\n",
    "    ci: confidence level\n",
    "    p: percentile' percent, for median it is 0.5\n",
    "    output: a list with two elements, [lowerBound, upperBound]\n",
    "    '''\n",
    "    if type(data) is pd.Series or type(data) is pd.DataFrame:\n",
    "        #transfer data into np.array\n",
    "        data = data.values\n",
    "    data = data.reshape(-1)\n",
    "    data = np.sort(data)\n",
    "    N = data.shape[0]\n",
    "    lowCount, upCount = stats.binom.interval(ci, N, p, loc=0)\n",
    "    #given this: https://onlinecourses.science.psu.edu/stat414/node/316\n",
    "    #lowCount and upCount both refers to  W's value, W follows binomial Dis.\n",
    "    #lowCount need to change to lowCount-1, upCount no need to change in python indexing\n",
    "  \n",
    "    #lowCount -= 1\n",
    "    upCount -=1\n",
    "    \n",
    "   \n",
    "    return data[int(lowCount)], data[int(upCount)]\n",
    "\n",
    "out = None\n",
    "\n",
    "n = len(rsa_dict.keys())\n",
    "ks = list(range(1, n + 1))                              #index\n",
    "\n",
    "\n",
    "vs = []\n",
    "for v in rsa_dict.values():\n",
    "    vs.append(medianvs(v))\n",
    "print(rsa_dict)\n",
    "palettes = {}\n",
    "c = 1\n",
    "for v in vs:\n",
    "    if v <= 5:\n",
    "        palettes[c] = \"Core\"\n",
    "    elif v > 25:\n",
    "        palettes[c] = \"Part\"\n",
    "    else:\n",
    "        palettes[c] = \"Surf\"\n",
    "    c +=1\n",
    "    \n",
    "legend_dict = { 'Core' : 'purple', 'Part' : 'orange', 'Surf' : 'royalblue' }\n",
    "\n",
    "patchList = []\n",
    "for key in legend_dict:\n",
    "    data_key = mpatches.Patch(facecolor=legend_dict[key], label=key, edgecolor = \"black\", linewidth = 3)\n",
    "    patchList.append(data_key)    \n",
    "i = 1\n",
    "lowerbound = []\n",
    "upperbound = []\n",
    "\n",
    "\n",
    "medianvs(rsa_dict)\n",
    "med = medianvs(rsa_dict)\n",
    "for v in rsa_dict.values():\n",
    "    \n",
    "    med = medianvs(v)\n",
    "    data = pd.Series(v)\n",
    "    low, up = medianCI(data, 0.95, 0.5)\n",
    "    lowerbound.append(med-low)\n",
    "    upperbound.append(up-med)\n",
    "    if v == 0:\n",
    "        print(i,v)\n",
    "    i = i+1\n",
    "\n",
    "cols = ['vs', 'palettes', 'lowerbound', 'upperbound' ]\n",
    "lst = []\n",
    "for a in ks:\n",
    "    \n",
    "    lst.append([vs[a-1], palettes[a], lowerbound[a-1], upperbound[a-1], ])\n",
    "df1 = pd.DataFrame(lst, columns=cols)\n",
    "\n",
    "hue_order=[\"purple\", \"royalblue\", \"orange\"]\n",
    "\n",
    "ax=sns.barplot(df1, y=df1.vs, x=df1.index, linewidth = 0.01,  hue =df1.palettes, palette=[\"purple\", \"royalblue\", \"orange\"]) #, ci=[df1.lowerbound, df1.upperbound]) #, errcolor = \"black\", errwidth = 7.5, capsize = 35 )\n",
    "plt.errorbar([k-1 for k in ks], vs, yerr=[lowerbound, upperbound], c = \"grey\", linewidth = 1, linestyle=\"None\", capsize = 1, capthick = 0.5)\n",
    "sns.set(rc={'figure.figsize':(50,50)})\n",
    "#ax.set_title('Median RSA per occupancy position', pad =10, fontsize = 40)\n",
    "ax.tick_params(axis= 'both' , which = 'major', labelsize = 30) \n",
    "ax.set_xlabel(\"Occupancy residue position\", labelpad = 10, fontsize = 60)\n",
    "ax.set_ylabel('Median RSA', labelpad = 10, fontsize = 60)\n",
    "ax.axhline(25, linewidth = 2, color = 'black', linestyle = '--')\n",
    "ax.axhline(5, linewidth = 2, color = 'black', linestyle = '--')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.yticks(np.arange(0, 105, 5))\n",
    "plt.xticks(np.arange(0, n, 10))\n",
    "plt.ylim(0, 100)  \n",
    "ax.margins(x=0)\n",
    "legend = plt.legend(handles=patchList, loc='center left', bbox_to_anchor=(1, 0.5),fontsize = 40)\n",
    "legend.get_frame().set_linewidth(2)\n",
    "legend.get_frame().set_edgecolor(\"black\")\n",
    "plt.figure(figsize=(180,80))\n",
    "for legobj in legend.legendHandles:\n",
    "    legobj.set_linewidth(1)\n",
    "if out != None:\n",
    "    plt.savefig(out)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_class_consensus(df, aln_cols):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_unique = df.drop_duplicates(['UniProt_dbAccessionId_A', 'PDB_dbAccessionId_A', 'PDB_dbChainId_A', 'PDB_dbResNum_A'])\n",
    "    df_unique = df_unique.dropna(subset = [\"SS_CLASS_A\"])\n",
    "    df_unique.Alignment_column_A = df_unique.Alignment_column_A.astype(float).astype(int)\n",
    "    df_cons_cols = df_unique[df_unique.Alignment_column_A.isin(aln_cols)]\n",
    "    missing = []\n",
    "    for x in aln_cols:\n",
    "        if x in df_cons_cols['Alignment_column_A'].values:\n",
    "            t = 0\n",
    "        else: \n",
    "            missing.append(x)\n",
    "\n",
    "\n",
    "    #get_struc_info(df_cons_cols)\n",
    "    grouped_col_rep = df_cons_cols.groupby([\"Alignment_column_A\", \"SOURCE_ID_A\"])\n",
    "    ss_class_dict = {}\n",
    "    i = 1\n",
    "    for k, v in grouped_col_rep:\n",
    "        col = k[0]\n",
    "        if col not in ss_class_dict:\n",
    "            #print(\"Processing consensus column {}\".format(i))\n",
    "            i += 1\n",
    "            ss_class_dict[col] = []\n",
    "        \n",
    "        for x in missing:\n",
    "            ss_class_dict[x] = []\n",
    "            \n",
    "        \n",
    "        ss_class_list = v.SS_CLASS_A.tolist()\n",
    "\n",
    "        try:\n",
    "            ss_class_cons = statistics.mode(ss_class_list)\n",
    "        except:\n",
    "            ss_class_cons = ss_class_list[0]\n",
    "        ss_class_dict[col].append(ss_class_cons)\n",
    "        ss_class_dict = dict(sorted(ss_class_dict.items()))\n",
    "    return ss_class_dict\n",
    "ss_class_dict = get_ss_class_consensus(df_allsp_rsrz_filt, cons_cols_allsp) # EXTRACTS SS DATA FROM TABLE AND RETURNS DICTIONARY WITH OCCURRENCE OF SS CLASS PER POSTITION\n",
    "print(len(ss_class_dict))\n",
    "print(ss_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_class_df(rsa_class_dict):\n",
    "    n = len(rsa_class_dict.keys())\n",
    "    h = []\n",
    "    c = []\n",
    "    e = []\n",
    "    for v in rsa_class_dict.values():\n",
    "        h.append(v.count(\"H\"))\n",
    "        c.append(v.count(\"C\"))\n",
    "        e.append(v.count(\"E\")) \n",
    "    df_dssp = pd.DataFrame(list(zip(h, c, e)), columns =  [\"helix\", \"coil\", \"strand\"])\n",
    "    df_dssp[\"tot\"] = df_dssp.helix + df_dssp.coil + df_dssp.strand\n",
    "    df_dssp[\"p_helix\"] = df_dssp.helix / df_dssp.tot\n",
    "    df_dssp[\"p_coil\"] = df_dssp.coil / df_dssp.tot\n",
    "    df_dssp[\"p_strand\"] = df_dssp.strand / df_dssp.tot\n",
    "    df_dssp[\"p_tot\"] = df_dssp.p_helix + df_dssp.p_coil + df_dssp.p_strand\n",
    "    df_dssp.index = range(1, n + 1)\n",
    "    return df_dssp\n",
    "ss_class_df = get_ss_class_df(ss_class_dict) # CALCULATES PROPORTION OF EACH SECONDARY STRUCTURE CLASS PER POISITION\n",
    "print(len(ss_class_df))\n",
    "print(ss_class_df)\n",
    "ss_class_df.loc[98]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ss_class_consensus(df_dssp, palette = [\"purple\", \"sienna\", \"cornflowerblue\"], bwidth = 1.2, out = None):\n",
    "    helix = list(df_dssp.p_helix)\n",
    "    coil = list(df_dssp.p_coil)\n",
    "    strand = list(df_dssp.p_strand)\n",
    "\n",
    "    n_cols = len(df_dssp)\n",
    "    print(n_cols)\n",
    "    bottom = []\n",
    "    for i in range(0, len(helix)):\n",
    "        bottom.append(strand[i]+helix[i])\n",
    "\n",
    "    r = list(np.arange(0,n_cols,1))\n",
    "    plt.figure(figsize=(180,80))\n",
    "    plt.rcParams.update({\"axes.linewidth\": 10})\n",
    "    plt.bar(r, strand,  color=palette[0], edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Strand')\n",
    "    plt.bar(r, helix, color=palette[1], bottom = strand, edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Helix')\n",
    "    plt.bar(r, coil,  bottom = bottom, color=palette[2], edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Loop')\n",
    "    plt.xlabel('Occupancy residue position', labelpad = 50, fontsize = 120)\n",
    "    plt.ylabel('p', labelpad = 50, fontsize = 120)\n",
    "    legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize = 120)\n",
    "    legend.get_frame().set_linewidth(10)\n",
    "    legend.get_frame().set_edgecolor(\"black\")\n",
    "    for legobj in legend.legendHandles:\n",
    "        legobj.set_linewidth(7.5)\n",
    "    plt.tick_params(axis= 'both' , which = 'major', pad = 30, width = 15, length = 50, labelsize = 80)\n",
    "    plt.axhline(y=0.5, linewidth = 5, linestyle = \"--\", color = 'black')\n",
    "    plt.yticks(np.arange(0,1.1,0.1))\n",
    "    plt.xticks(np.arange(0, n_cols, 20))\n",
    "    plt.xlim(0, n_cols) \n",
    "    plt.ylim(0, 1.025)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_ss_class_consensus(ss_class_df, palette = [\"purple\", \"sienna\", \"cornflowerblue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_consensus(df, aln_cols):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_unique = df.drop_duplicates(['UniProt_dbAccessionId_A', 'PDB_dbAccessionId_A', 'PDB_dbChainId_A', 'PDB_dbResNum_A'])\n",
    "    df_unique = df_unique.dropna(subset = [\"SS_A\"])\n",
    "    df_unique.Alignment_column_A = df_unique.Alignment_column_A.astype(float).astype(int)\n",
    "    df_cons_cols = df_unique[df_unique.Alignment_column_A.isin(aln_cols)]\n",
    "\n",
    "    missing = []\n",
    "    for x in aln_cols:\n",
    "        if x in df_cons_cols['Alignment_column_A'].values:\n",
    "            t = 0\n",
    "        else: \n",
    "            missing.append(x)\n",
    "\n",
    "\n",
    "\n",
    "    #get_struc_info(df_cons_cols)\n",
    "    grouped_col_rep = df_cons_cols.groupby([\"Alignment_column_A\", \"SOURCE_ID_A\"])\n",
    "    ss_class_dict = {}\n",
    "    i = 1\n",
    "    for k, v in grouped_col_rep:\n",
    "        col = k[0]\n",
    "        if col not in ss_class_dict:\n",
    "            #print(\"Processing consensus column {}\".format(i))\n",
    "            i += 1\n",
    "            ss_class_dict[col] = []\n",
    "\n",
    "        for x in missing:\n",
    "            ss_class_dict[x] = []\n",
    "\n",
    "        ss_class_list = v.SS_A.tolist()\n",
    "        try:\n",
    "            ss_class_cons = statistics.mode(ss_class_list)\n",
    "        except:\n",
    "            ss_class_cons = ss_class_list[0]\n",
    "        ss_class_dict[col].append(ss_class_cons)\n",
    "        ss_class_dict = dict(sorted(ss_class_dict.items()))\n",
    "    return ss_class_dict\n",
    "\n",
    "ss_dict = get_ss_consensus(df_allsp_rsrz_filt, cons_cols_allsp) # EXTRACTS SECONDARY STRUCTURE DATA FROM TABLE AND RETURNS DICTIONARY WITH OCCURRENCE OF SS ELEMENTS PER POSTITION\n",
    "print(ss_dict)\n",
    "print(len(ss_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_df(ss_class_dict):\n",
    "    n = len(ss_class_dict.keys())\n",
    "    h = []\n",
    "    b = []\n",
    "    e = []\n",
    "    g = []\n",
    "    i = []\n",
    "    t = []\n",
    "    s = []\n",
    "    c = []\n",
    "    for v in ss_class_dict.values():\n",
    "        h.append(v.count(\"H\"))\n",
    "        b.append(v.count(\"B\"))\n",
    "        e.append(v.count(\"E\"))\n",
    "        g.append(v.count(\"G\"))\n",
    "        i.append(v.count(\"I\"))\n",
    "        t.append(v.count(\"T\")) \n",
    "        s.append(v.count(\"S\"))\n",
    "        c.append(v.count(\"\"))\n",
    "    df_dssp = pd.DataFrame(list(zip(h,b,e,g,i,t,s,c)), columns =  [\"a_helix\",\"b_bridge\",\"strand\",\"helix_3_10\", \"pi_helix\",\"turn\",\"bend\",\"coil\"])\n",
    "    df_dssp[\"tot\"] = df_dssp.a_helix + df_dssp.b_bridge + df_dssp.strand+df_dssp.helix_3_10 + df_dssp.pi_helix + df_dssp.turn+ df_dssp.bend + df_dssp.coil\n",
    "    df_dssp[\"p_a_helix\"] = df_dssp.a_helix / df_dssp.tot\n",
    "    df_dssp[\"p_b_bridge\"] = df_dssp.b_bridge / df_dssp.tot\n",
    "    df_dssp[\"p_strand\"] = df_dssp.strand / df_dssp.tot\n",
    "    df_dssp[\"p_3_10_helix\"] = df_dssp.helix_3_10 / df_dssp.tot\n",
    "    df_dssp[\"p_pi_helix\"] = df_dssp.pi_helix / df_dssp.tot\n",
    "    df_dssp[\"p_turn\"] = df_dssp.turn / df_dssp.tot\n",
    "    df_dssp[\"p_bend\"] = df_dssp.bend / df_dssp.tot\n",
    "    df_dssp[\"p_coil\"] = df_dssp.coil / df_dssp.tot\n",
    "    df_dssp[\"p_tot\"] = df_dssp.p_a_helix + df_dssp.p_b_bridge + df_dssp.p_strand+df_dssp.p_3_10_helix + df_dssp.p_pi_helix +df_dssp.p_turn + df_dssp.p_bend + df_dssp.p_coil\n",
    "    df_dssp.index = range(1, n + 1)\n",
    "    return df_dssp\n",
    "\n",
    "ss_df = get_ss_df(ss_dict) # CALCULATES PROPORTION OF EACH SECONDARY STRUCTURE ELEMENT PER POISITION\n",
    "print(len(ss_df))\n",
    "print(ss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ss_consensus(df_dssp, palette = [\"purple\", \"darkviolet\", \"violet\", \"sienna\",\"sandybrown\",\"grey\",\"blue\",\"cornflowerblue\"], bwidth = 1.2, out = None):\n",
    "    a_helix = list(df_dssp.p_a_helix)\n",
    "    b_bridge = list(df_dssp.p_b_bridge)\n",
    "    strand = list(df_dssp.p_strand)\n",
    "    helix_3_10 = list(df_dssp.p_3_10_helix)\n",
    "    pi_helix = list(df_dssp.p_pi_helix)\n",
    "    turn = list(df_dssp.p_turn)\n",
    "    bend = list(df_dssp.p_bend)\n",
    "    coil = list(df_dssp.p_coil)\n",
    "    n_cols = len(df_dssp)\n",
    "    bottom = [0 for i in range(0, n_cols)]\n",
    "    #print(bottom)\n",
    "    #print(len(bottom))\n",
    "    r = list(np.arange(0,n_cols,1))\n",
    "    plt.figure(figsize=(180,80))\n",
    "    plt.rcParams.update({\"axes.linewidth\": 10})\n",
    "    plt.bar(r, a_helix,  color=palette[0], bottom = bottom, edgecolor='black', linewidth = 7.5, width=bwidth, label = r'$\\alpha$-helix')\n",
    "    for i in range(0, n_cols):\n",
    "        bottom[i] += a_helix[i]\n",
    "    plt.bar(r, helix_3_10, color=palette[1], bottom = bottom,  edgecolor='black', linewidth = 7.5, width=bwidth, label = r'$3_{10}$-helix')\n",
    "    for i in range(0, n_cols):\n",
    "        bottom[i] += helix_3_10[i]\n",
    "    plt.bar(r, pi_helix,  color=palette[2], bottom = bottom,  edgecolor='black', linewidth = 7.5, width=bwidth, label = r'$\\pi$-helix')\n",
    "    for i in range(0, n_cols):\n",
    "        bottom[i] += pi_helix[i]\n",
    "    plt.bar(r, b_bridge,  color=palette[3], bottom = bottom, edgecolor='black', linewidth = 7.5, width=bwidth, label = r'$\\beta$-bridge')\n",
    "    for i in range(0, n_cols):\n",
    "        bottom[i] += b_bridge[i]\n",
    "    plt.bar(r, strand, color=palette[4], bottom = bottom, edgecolor='black', linewidth = 7.5, width=bwidth, label = r'$\\beta$-strand')\n",
    "    for i in range(0, n_cols):\n",
    "        bottom[i] += strand[i]\n",
    "    plt.bar(r, turn,  color=palette[5], bottom = bottom, edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Turn')\n",
    "    for i in range(0, n_cols):\n",
    "        bottom[i] += turn[i]\n",
    "    plt.bar(r, bend,  color=palette[6], bottom = bottom,edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Bend')\n",
    "    for i in range(0, n_cols):\n",
    "        bottom[i] += bend[i]\n",
    "    plt.bar(r, coil, color=palette[7], bottom = bottom, edgecolor='black', linewidth = 7.5, width=bwidth, label = 'Coil')\n",
    "    plt.xlabel('Domain position', labelpad = 50, fontsize = 120)\n",
    "    plt.ylabel('p', labelpad = 50, fontsize = 120)\n",
    "    legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize = 120)\n",
    "    #legend = ax.legend()\n",
    "    legend.get_frame().set_linewidth(10)\n",
    "    legend.get_frame().set_edgecolor(\"black\")\n",
    "    for legobj in legend.legendHandles:\n",
    "        legobj.set_linewidth(7.5)\n",
    "    plt.tick_params(axis= 'both' , which = 'major', pad = 30, width = 15, length = 50, labelsize = 80)\n",
    "    plt.axhline(y=0.5, linewidth = 7.5, linestyle = \"--\", color = 'black')\n",
    "    plt.yticks(np.arange(0,1.1,0.1))\n",
    "    plt.xticks(np.arange(0, n_cols, 20))\n",
    "    plt.xlim(0, n_cols) \n",
    "    plt.ylim(0, 1.025)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "plot_ss_consensus(ss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTACT MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import structural_analysis\n",
    "import statistics\n",
    "#import structure_validation\n",
    "import importlib\n",
    "from scipy import stats\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path  \n",
    "import vcf\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_allsp = pd.read_pickle(r\"C:\\Users\\Adam Martin\\ONEDRI~1\\HONOUR~1\\JALVIE~1\\UNIPRO~1\\UNIPRO~1.GZ\")\n",
    "df_allsp = pd.read_pickle(r'/homes/2414054/varalign_folder/Altered_Alignment/Changing_Alignment/Uniprot_Annotated_Alignment127_PFAM.sto_prointvar_structure_table.p.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allsp.shape\n",
    "df_allsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_in = \"/homes/2414054/varalign_folder/Altered_Alignment/Changing_Alignment/Alignment127_PFAM.sto\" # MSA\n",
    "#aln_in = r\"C:\\Users\\Adam Martin\\OneDrive - University of Dundee\\Honours Project\\Jalview and Chimera using the alignment from PF00026 first from linux\\Uniprot_Sequences_For_Alignment_To_Allow_VarAlign_TO\\Uniprot_Annotated_Alignment127_PFAM.sto\"\n",
    "  \n",
    "aln_fmt = \"stockholm\" # MSA FORMAT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import Bio.SeqIO\n",
    "import Bio.AlignIO\n",
    "import statistics\n",
    "import importlib\n",
    "from Bio.PDB import *\n",
    "\n",
    "def get_n_seq(seq_file, seq_format = \"fasta\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    seqs = Bio.SeqIO.parse(seq_file,seq_format)\n",
    "    n = 0\n",
    "    for seq in seqs:\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "def get_occ_cols(aln_in, fmt_in): #returns a dictionary of the occupoancy of every column in the alignment\n",
    "    aln = Bio.SeqIO.parse(aln_in, fmt_in)\n",
    "    occ = {}\n",
    "    for rec in aln:\n",
    "        seq = str(rec.seq)\n",
    "        for i in range(0, len(seq)):\n",
    "            if i + 1 not in occ:\n",
    "                occ[i+1] = 0\n",
    "            if seq[i] != '-':\n",
    "                occ[i+1] += 1\n",
    "    return occ\n",
    "\n",
    "def get_cons_cols(aln_in, fmt_in, t = 0.5): #returns the columns with a relative occupancy greater than a threshold\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_seq = get_n_seq(aln_in, fmt_in)\n",
    "    aln_occ = get_occ_cols(aln_in, fmt_in)\n",
    "    cons_cols = [col for col, occ in aln_occ.items() if occ/n_seq > t]\n",
    "    return cons_cols\n",
    "\n",
    "\n",
    "cons_cols_allsp = get_cons_cols(aln_in, aln_fmt) # GETS CONSENSUS COLUMNS FROM MSA (OCCUPANCY > 0.5)\n",
    "print(len(cons_cols_allsp))\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cons_cols_eq(cons_cols):\n",
    "    cons_cols_eq = {}\n",
    "    for i, col in enumerate(cons_cols):\n",
    "        cons_cols_eq[col] = i+1\n",
    "    return cons_cols_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df, cons_cols):\n",
    "\tcons_cols_eq = get_cons_cols_eq(cons_cols)\n",
    "\tdf2 = df.copy(deep = True)\n",
    "\tdf2_filt = df2.dropna(subset = [\"Alignment_column_A\", \"Alignment_column_B\"])\n",
    "\tdf2_filt.Alignment_column_A = df2_filt.Alignment_column_A.astype(int)\n",
    "\tdf2_filt.Alignment_column_B = df2_filt.Alignment_column_B.astype(int)\n",
    "\tdf2_filt.UniProt_dbResNum_A = df2_filt.UniProt_dbResNum_A.astype(int)\n",
    "\tdf2_filt.UniProt_dbResNum_B = df2_filt.UniProt_dbResNum_B.astype(int)\n",
    "\tdf2_cons = df2_filt[(df2_filt.Alignment_column_A.isin(cons_cols)) & (df2_filt.Alignment_column_B.isin(cons_cols))]\n",
    "\tdf2_cons[\"Alignment_column_cons_A\"] = df2_cons.Alignment_column_A.map(cons_cols_eq)\n",
    "\tdf2_cons[\"Alignment_column_cons_B\"] = df2_cons.Alignment_column_B.map(cons_cols_eq)\n",
    "\treturn df2_cons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = format_df(df_allsp, cons_cols_allsp) # FORMATS STRUCTURAL TABLE FOR FOLLOW-UP ANALYSIS\n",
    "df_rf\t\t\t                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_cols_eq(df):\n",
    "    cols = df.columns.tolist()\n",
    "    un_cols = list(set([col[:-2] for col in cols]))\n",
    "    cols_eq = {}\n",
    "    for col in cols:\n",
    "        if col[-2:] == \"_A\":\n",
    "            cols_eq[col] = col[:-2] + \"_B\"\n",
    "        elif col[-2:] == \"_B\":\n",
    "            cols_eq[col] = col[:-2] + \"_A\"\n",
    "        else:\n",
    "            cols_eq[col] = col\n",
    "    return cols_eq\n",
    "\n",
    "cons_cols_eq = get_cons_cols_eq(cons_cols_allsp) # EQUIVALENCE BETWEEN CONSENSUS ALIGNMENT COLUMN NUMBERS AND DOMAIN POSITIONS (1-33)\n",
    "print(cons_cols_eq)\n",
    "df_cols_eq = get_df_cols_eq(df_rf)\n",
    "print(df_cols_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_res_in_rep(df, rep_id):\n",
    "#    df_rep = df[(df.SOURCE_ID_A == rep_id)&(df.SOURCE_ID_B == rep_id)]\n",
    "#    res_a = df_rep.Alignment_column_A.unique().tolist()                         #maybe this should be concensus instead of alignment column\n",
    "#    res_b = df_rep.Alignment_column_B.unique().tolist()\n",
    "#    res = sorted(list(set(res_a + res_b)))\n",
    "#    return res\n",
    "\n",
    "def get_res_in_rep(df, rep_id, cons_eq):\n",
    "    df_rep = df[(df.SOURCE_ID_A == rep_id)&(df.SOURCE_ID_B == rep_id)]\n",
    "    res_a = df_rep.Alignment_column_A.unique().tolist()\n",
    "    res_b = df_rep.Alignment_column_B.unique().tolist()\n",
    "    res = sorted(list(set(res_a + res_b)))\n",
    "    cons_res = [cons_eq[r] for r in res]\n",
    "    return cons_res\n",
    "\n",
    "\n",
    "\n",
    "def get_struc_info(df):\n",
    "    n_res = len(df.drop_duplicates(['UniProt_dbAccessionId_A', 'PDB_dbAccessionId_A', 'PDB_dbChainId_A', 'PDB_dbResNum_A']))\n",
    "    n_res_un = len(df.drop_duplicates([\"SOURCE_ID_A\", \"Alignment_column_A\"]))\n",
    "    n_strucs =  len(df.PDB_dbAccessionId_A.unique().tolist())\n",
    "    n_prots = len(df.UniProt_dbAccessionId_A.unique().tolist())\n",
    "    n_reps = len(df.SOURCE_ID_A.unique().tolist())\n",
    "    print(\"The dataframe contains information of:\\n{} residues, {} of which are unique\\n{} PDB structures\\n{} different proteins\\n{} unique repeats\".format(n_res, n_res_un, n_strucs, n_prots, n_reps))\n",
    "    \n",
    "\n",
    "#def get_intra_cons_occ(df, cons_cols_eq):\n",
    "#    reps = df.SOURCE_ID_A.unique().tolist()\n",
    "#    res_occ = {}\n",
    "#    idseq = []\n",
    "#    for rep in reps:\n",
    "#        rep_res = get_res_in_rep(df, rep)\n",
    "#        #print(rep_res)\n",
    "#        for i in rep_res:\n",
    "#            for j in rep_res:\n",
    "#                idx = tuple([cons_cols_eq[i], cons_cols_eq[j]])\n",
    "#                if idx not in res_occ:\n",
    "#                    res_occ[idx] = 0\n",
    "#                res_occ[idx] += 1\n",
    "#                #if i not in idseq: \n",
    "#                #    idseq.append(i)\n",
    "#                #if j not in idseq: \n",
    "#                #    idseq.append(j)\n",
    "#        #idseq.sort()\n",
    "#    return res_occ, #idseq\n",
    "\n",
    "\n",
    "\n",
    "def get_intra_cons_occ(df, cons_eq):\n",
    "    get_struc_info(df)\n",
    "    reps = df.SOURCE_ID_A.unique().tolist()\n",
    "    res_occ = {}\n",
    "    for rep in reps:\n",
    "        rep_res = get_res_in_rep(df, rep, cons_eq)\n",
    "        for i in rep_res:\n",
    "            for j in rep_res:\n",
    "                idx = tuple([i, j])\n",
    "                if idx not in res_occ:\n",
    "                    res_occ[idx] = 0\n",
    "                res_occ[idx] += 1\n",
    "    return res_occ\n",
    "\n",
    "\n",
    "\n",
    "intra_res_occ  = get_intra_cons_occ(df_rf, cons_cols_eq) # GETS COVERAGE OF EACH INTRA-REPEAT POSITION PAIR IN OUR STRUCTURAL DATASET\n",
    "\n",
    "intra_res_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_contact_matrix(df):\n",
    "    df2 = pd.DataFrame.copy(df, deep = True)\n",
    "    rows = list(df.index)\n",
    "    cols = list(df.columns)\n",
    "    for i in rows:\n",
    "        for j in cols[i-1:]:\n",
    "            df2.loc[j,i] = df2.loc[i,j]\n",
    "    return df2\n",
    "\n",
    "#def symmetric_contact_matrix(df, cons_cols):\n",
    "#    df2 = pd.DataFrame.copy(df, deep = True)\n",
    "#    rows = list(df.index)\n",
    "#    cols = list(df.columns)\n",
    "#    x=0\n",
    "#    for i in rows:\n",
    "#        x +=1\n",
    "#        for j in cols[x-1:]:\n",
    "#            #if  df2.loc[i,j] > 0:\n",
    "#                #print(i,j, df2.loc[i,j])\n",
    "#            df2.loc[j,i] = df2.loc[i,j]\n",
    "#    return df2\n",
    "\n",
    "#def get_intra_cons(df, cons_cols_eq, t = 0, int_mask = None):\n",
    "#    real_cols = list(cons_cols_eq.keys())\n",
    "#    cons_cols = list(cons_cols_eq.values())\n",
    "#    df2 = df.copy(deep = True)\n",
    "#    if int_mask != None:\n",
    "#        df2 = df2[df2.Int_Types.str.contains(int_mask)]\n",
    "#    contact_matrix_intra = pd.DataFrame(np.nan, index = cons_cols, columns = cons_cols).fillna(0)\n",
    "#    if t == 0:\n",
    "#        df_filt = df2[df2.SOURCE_ID_A == df2.SOURCE_ID_B].groupby(\"SOURCE_ID_A\")\n",
    "#    else:\n",
    "#        df_filt = df2[(df.SOURCE_ID_A == df2.SOURCE_ID_B) & (abs(df2.Alignment_column_cons_A - df2.Alignment_column_cons_B) > t)].groupby(\"SOURCE_ID_A\")\n",
    "#    for repeat, row in df_filt:\n",
    "#        repeat_crosstab = pd.crosstab(row.Alignment_column_cons_A,\n",
    "#                                      row.Alignment_column_cons_B)    # creates a cross table of how many contacts there are between any two given residues\n",
    "#        repeat_crosstab = repeat_crosstab.reindex(cons_cols).fillna(0)         # changes index (rows) so it includes all positions (1-33)\n",
    "#        repeat_crosstab = repeat_crosstab.reindex(columns = cons_cols).fillna(0) # changes column index so it includes all positions (1-33)\n",
    "#        repeat_crosstab[repeat_crosstab >= 1] = 1 # flattening all interatomic interactions and overrepresented interactions in multiple structure to value of one\n",
    "#        contact_matrix_intra += repeat_crosstab  # adding each crosstab for each repeat to the contact matrix dataframe to study all interactions\n",
    "#    contact_matrix_intra_symm = symmetric_contact_matrix(contact_matrix_intra)\n",
    "#    return contact_matrix_intra_symm\n",
    "\n",
    "def get_intra_cons(df, cons_cols_eq, t = 0, int_mask = None):\n",
    "    real_cols = list(cons_cols_eq.keys())\n",
    "    cons_cols = list(cons_cols_eq.values())\n",
    "    df2 = df.copy(deep = True)\n",
    "    if int_mask != None:\n",
    "        df2 = df2[df2.Int_Types.str.contains(int_mask)]\n",
    "    contact_matrix_intra = pd.DataFrame(np.nan, index = cons_cols, columns = cons_cols).fillna(0)\n",
    "    if t == 0:\n",
    "        df_filt = df2[df2.SOURCE_ID_A == df2.SOURCE_ID_B].groupby(\"SOURCE_ID_A\")\n",
    "    else:\n",
    "        df_filt = df2[(df.SOURCE_ID_A == df2.SOURCE_ID_B) & (abs(df2.Alignment_column_cons_A - df2.Alignment_column_cons_B) > t)].groupby(\"SOURCE_ID_A\")\n",
    "    for repeat, row in df_filt:\n",
    "        repeat_crosstab = pd.crosstab(row.Alignment_column_cons_A,\n",
    "                                      row.Alignment_column_cons_B)    # creates a cross table of how many contacts there are between any two given residues\n",
    "        repeat_crosstab = repeat_crosstab.reindex(cons_cols).fillna(0)         # changes index (rows) so it includes all positions (1-33)\n",
    "        repeat_crosstab = repeat_crosstab.reindex(columns = cons_cols).fillna(0) # changes column index so it includes all positions (1-33)\n",
    "        repeat_crosstab[repeat_crosstab >= 1] = 1 # flattening all interatomic interactions and overrepresented interactions in multiple structure to value of one\n",
    "        contact_matrix_intra += repeat_crosstab  # adding each crosstab for each repeat to the contact matrix dataframe to study all interactions\n",
    "    print(contact_matrix_intra)\n",
    "    contact_matrix_intra_symm = symmetric_contact_matrix(contact_matrix_intra)\n",
    "    return contact_matrix_intra_symm    \n",
    "\n",
    "\n",
    "intra = get_intra_cons(df_rf, cons_cols_eq, t = 0) # CALCULATES ABSOLUTE FREQUENCY OF INTRA-REPEAT CONTACTS                 #Changed the consensus sequences used to one using just intra make the matrix and graph look better\n",
    "intra                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def normalize_contacts(df, res_occ_dict):\n",
    "#    max = 0\n",
    "#    max2=0\n",
    "#    df2 = df.copy(deep = True)\n",
    "#    df3 = df.copy(deep = True)\n",
    "#    for k, v in res_occ_dict.items():                                                                       #res_occ_dict is missing the start and end data. 134 is supposed to 7 and 968 is supossed to be 7\n",
    "#        \n",
    "#        if k[0] > max:\n",
    "#            max = k[0]\n",
    "#            #print(\"max\", max)\n",
    "#        if k[1] > max2:\n",
    "#            max2 = k[1]\n",
    "#            #print(\"max\", max)#\n",
    "#\n",
    "#        df3.loc[k[1],k[0]] = df2.loc[k[0],k[1]]/v\n",
    "#        df3.loc[k[0],k[1]] = 0                            \n",
    "#        \n",
    "#    \n",
    "##   \n",
    " #   \n",
    " #   print(\"max\", max, max2)\n",
    " #   return df3\n",
    "\n",
    "\n",
    "def normalize_contacts(df, res_occ_dict, Half=\"False\"):\n",
    "    df2 = df.copy(deep = True)\n",
    "    df3 = df.copy(deep = True)\n",
    "    for k, v in res_occ_dict.items():\n",
    "        \n",
    "        df3.loc[k[1],k[0]] = df2.loc[k[0],k[1]]/v\n",
    "        if Half==\"True\":\n",
    "            df3.loc[k[0],k[1]] = 0                            \n",
    "    return df3\n",
    "\n",
    "intra_norm = normalize_contacts(intra, intra_res_occ, Half=\"False\") # NORMALISES CONTACT MAP BY POSITION PAIR COVERAGE IN DATASET\n",
    "\n",
    "# Sorted out the normalising bit, seemed to be out of indexing. Now not sure about the 134 bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def plot_intra_cons(df, df_cols_eq, cmap = 'Greens', out = None):\n",
    "#    \n",
    "#    n_cols = len(df)\n",
    "#    print(n_cols)\n",
    "#    fig, ax = plt.subplots(1, 1, figsize=(200, 160))\n",
    "#    ax = sns.heatmap(df, cmap = cmap, square = True, linewidths = 5, linecolor = 'black', cbar_kws = dict(ticks = np.arange(0,1,1)))\n",
    "#    ax.xaxis.tick_top()\n",
    "#    ax.yaxis.tick_left()\n",
    "#    ax.tick_params(axis= 'both' , labelrotation = 'auto', which = 'major', pad = 60, width = 15, length = 50, labelsize = 80)\n",
    "#    ax.figure.axes[-1].yaxis.label.set_size(180)\n",
    "#    cbar = ax.collections[0].colorbar\n",
    "#    cbar.set_label('p', labelpad=190)\n",
    "#    cbar.ax.tick_params(pad = 60, width = 5, length = 30, labelsize = 180)\n",
    "##    plt.xlabel(\"Occupancy residue position\", fontsize = 120, labelpad = 100)\n",
    "#    plt.ylabel(\"Occupancy residue position\", fontsize = 120, labelpad = 100)\n",
    "##    plt.xticks(np.arange(0, n_cols, 20))\n",
    " #   plt.yticks(np.arange(0, n_cols, 20))\n",
    " #   ax.xaxis.set_label_position('top')\n",
    "#    if out != None:\n",
    "#        plt.savefig(out)\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "def plot_intra_cons(df, cmap = 'Greens', out = None):\n",
    "    rage = np.arange(0, len(df), 20)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(200, 160))\n",
    "    ax = sns.heatmap(df, cmap = cmap, square = True, linewidths = 5, linecolor = 'black', cbar_kws = dict(ticks = np.arange(0,1.1,0.1)))\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.set_xticks(np.arange(0, len(df), 20), labels= rage)\n",
    "    ax.set_yticks(np.arange(0, len(df), 20), labels= rage)\n",
    "    ax.tick_params(axis= 'both' , labelrotation = 'auto', which = 'major', pad = 60, width = 5, length = 30, labelsize = 180)\n",
    "    ax.figure.axes[-1].yaxis.label.set_size(180)\n",
    "    ax.set_ylim(len(df),0)\n",
    "    ax.set_xlim(0, len(df))\n",
    "    \n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label('p', labelpad=190)\n",
    "    cbar.ax.tick_params(pad = 60, width = 5, length = 30, labelsize = 180)\n",
    "    plt.xlabel(\"Occupancy residue position\", fontsize = 180, labelpad = 60)\n",
    "    plt.ylabel(\"Occupancy residue position\", fontsize = 180, labelpad = 60)\n",
    "    plt.tick_params( which = 'major', pad = 30, width = 15, length = 50, labelsize = 80)\n",
    "    \n",
    "    ax.xaxis.set_label_position('top')\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "plot_intra_cons(intra_norm, \"viridis\")\n",
    "print(len(intra_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_t5 = get_intra_cons(df_rf, cons_cols_eq, t = 5) # ONLY PAIRS OF POSITIONS FURTHER AWAY THAN 5 POSITIONS FROM EACH OTHER\n",
    "intra_norm_t5 = normalize_contacts(intra_t5, intra_res_occ) # NORMALISATION\n",
    "\n",
    "print(intra_norm_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intra_cons(intra_norm_t5, \"viridis\")\n",
    "print(len(intra_norm_t5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tot_occ_res(occ_dict, n_cols, contact_map = \"intra\", t = 0):\n",
    "    res_occ = {}\n",
    "    for i in list(occ_dict.keys()):\n",
    "        if i[0] not in res_occ:\n",
    "            res_occ[i[0]] = 0\n",
    "        if contact_map == \"intra\":\n",
    "            if abs(i[1] - i[0]) > t:\n",
    "                res_occ[i[0]] += occ_dict[(i[0],i[0])]\n",
    "    for x in range(1,n_cols,1):\n",
    "        Found = \"False\"\n",
    "        for y in res_occ.keys():\n",
    "            if x == y:\n",
    "                Found = \"True\"\n",
    "        if Found == \"False\":\n",
    "            res_occ[x] = 0\n",
    "    res_occ = dict(sorted(res_occ.items()))\n",
    "    return res_occ\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_tot_cons_res(cons_df, contact_map = \"intra\", t = 0):\n",
    "    res_cons = {}\n",
    "    idx = cons_df.index.tolist()\n",
    "    for i in idx:\n",
    "        if i not in res_cons:\n",
    "            res_cons[i] = 0\n",
    "        for j in idx:\n",
    "            if contact_map == \"intra\":\n",
    "                \n",
    "                if abs(j - i) > t:\n",
    "                    res_cons[i] += cons_df.loc[i,j]\n",
    "            elif contact_map == \"inter\":\n",
    "                if i == j:\n",
    "                    res_cons[i] += cons_df.loc[i,j]\n",
    "                else:\n",
    "                    res_cons[i] += (cons_df.loc[i,j] + cons_df.loc[j,i])         \n",
    "    return res_cons\n",
    "\n",
    "def get_OR(df):\n",
    "    df.contacts = df.contacts + 1 #pseudocount\n",
    "    tot_occ = sum(df.occ)\n",
    "    tot_cons = sum(df.contacts)\n",
    "    for i in df.index:\n",
    "        i_occ = df.loc[i,\"occ\"]\n",
    "        i_cons = df.loc[i,'contacts']\n",
    "\n",
    "        rest_occ = tot_occ - i_occ\n",
    "        rest_cons = tot_cons - i_cons\n",
    "        \n",
    "        oddsr, pval = stats.fisher_exact([[i_cons, rest_cons], [i_occ, rest_occ]])\n",
    "        vals = [i_cons,rest_cons,i_occ, rest_occ]\n",
    "        se_logor = 1.96*(math.sqrt(sum(list(map((lambda x: 1/x),vals)))))\n",
    "        logor = math.log(oddsr)\n",
    "        df.loc[i,'oddsratio'] = oddsr\n",
    "        df.loc[i,'log_oddsratio'] = logor\n",
    "        df.loc[i,'pvalue'] = pval\n",
    "        df.loc[i,'ci_dist'] = se_logor\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_OR_from_cons(res_cons, res_occ):\n",
    "    cons = list(res_cons.values())\n",
    "    occ = list(res_occ.values())\n",
    "    df = pd.DataFrame(list(zip(occ, cons)), columns =['occ', 'contacts'])\n",
    "    df.index = range(1,len(df)+1)\n",
    "    df = get_OR(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_colours(df, missense_variants_df ):\n",
    "    jioner = missense_variants_df[['miss_class','miss_color']].copy()\n",
    "    df2 = pd.merge(df, jioner, left_index=True, right_index=True)\n",
    "    return df2\n",
    "\n",
    "\n",
    "def fisher_exact_test(df,x):\n",
    "    \n",
    "    cmd_cons = df[df.miss_class == x].contacts.sum()\n",
    "    cmd_occ = df[df.miss_class == x].occ.sum()\n",
    "    rest_cons = df.contacts.sum() - cmd_cons\n",
    "    rest_occ = df.occ.sum() - cmd_occ\n",
    "    oddsr1, pval1 = stats.fisher_exact([[cmd_cons, rest_cons], [cmd_occ, rest_occ]]) # CMDs ARE NOT ENRICHED IN INTER-REPEAT CONTACTS\n",
    "\n",
    "    cmd_cons = df[df.miss_class == x].contacts.sum()\n",
    "    cmd_occ = df[df.miss_class == x].occ.sum()\n",
    "    rest_cons = df[df.miss_class != x].contacts.sum()\n",
    "    rest_occ = df[df.miss_class != x].occ.sum()\n",
    "    oddsr2, pval2 = stats.fisher_exact([[cmd_cons, rest_cons], [cmd_occ, rest_occ]]) # CMDs ARE ENRICHED IN INTRA-REPEAT CONTACTS RELATIV\n",
    "    if oddsr1 == oddsr2 and pval1 == pval2:\n",
    "        print(x, oddsr1, pval1)\n",
    "    else:\n",
    "        print(x, oddsr1, pval1)\n",
    "        print()\n",
    "        print(x, oddsr2, pval2)\n",
    "    \n",
    "\n",
    "def add_miss_class(df, cons_col = \"shenkin\", thresholds = [30, 70], colours = [\"royalblue\", \"seagreen\", \"grey\", \"purple\", \"orange\"]):\n",
    "    for i in df.index:          \n",
    "        if df.loc[i, cons_col] <= thresholds[0] and df.loc[i, \"log_oddsratio\"] < 0:\n",
    "            df.loc[i,\"miss_class\"] = \"CMD\"\n",
    "        elif df.loc[i, cons_col] <= thresholds[0] and df.loc[i, \"log_oddsratio\"] > 0:\n",
    "            df.loc[i,\"miss_class\"] = \"CME\"\n",
    "        elif df.loc[i, cons_col] >= thresholds[1] and df.loc[i, \"log_oddsratio\"] < 0:\n",
    "            df.loc[i,\"miss_class\"] = \"UMD\"\n",
    "        elif df.loc[i, cons_col] >= thresholds[1] and df.loc[i, \"log_oddsratio\"] > 0:\n",
    "            df.loc[i,\"miss_class\"] = \"UME\"\n",
    "        else:\n",
    "            df.loc[i,\"miss_class\"] = \"None\"\n",
    "           \n",
    "    coloring = {\n",
    "        \"CMD\": colours[0],\n",
    "        \"CME\": colours[1],\n",
    "        \"UMD\": colours[3],\n",
    "        \"UME\": colours[4],\n",
    "        \"None\": colours[2]\n",
    "    }\n",
    "    df[\"miss_color\"] =  df.miss_class.map(coloring) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occ_intra = get_tot_occ_res(intra_res_occ, len(intra), contact_map = \"intra\", t = 5) # STRUCTURAL COVERAGE PER POSITION\n",
    "#cons_intra = get_tot_cons_res(intra, contact_map = \"intra\", t = 5) # TOTAL NUMBER OF CONTACTS PER POSITION\n",
    "#enrichment_df_intra = get_OR_from_cons(cons_intra, occ_intra) # CALCULATES ENRICHMENT SCORES IN INTRA-REPEAT CONTACTS, P-VALUE, AND 95% CI FOR ENRICHMENT SCORE\n",
    "#enrichment_df_intra = add_miss_class(enrichment_df_intra) # ADDS MISSENSE ENRICHMENT CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSERVATION AND VARIATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import Bio.SeqIO\n",
    "import Bio.AlignIO\n",
    "import statistics\n",
    "import importlib\n",
    "from Bio.PDB import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_table = pd.read_pickle(\"/homes/2414054/varalign_folder/Altered_Alignment/Changing_Alignment/Uniprot_Annotated_Alignment127_PFAM.sto_variants.p.gz\") # VARIANT TABLE\n",
    "aln_in = \"/homes/2414054/varalign_folder/Altered_Alignment/Changing_Alignment/Alignment127_PFAM.sto\" # MSA\n",
    "#aln_in = r\"C:\\Users\\Adam Martin\\OneDrive - University of Dundee\\Honours Project\\Jalview and Chimera using the alignment from PF00026 first from linux\\Uniprot_Sequences_For_Alignment_To_Allow_VarAlign_TO\\Uniprot_Annotated_Alignment127_PFAM.sto\"\n",
    "  \n",
    "aln_fmt = \"stockholm\" # MSA FORMAT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occ_cols(aln_in, fmt_in): #returns a dictionary of the occupoancy of every column in the alignment\n",
    "    aln = Bio.SeqIO.parse(aln_in, fmt_in)\n",
    "    occ = {}\n",
    "    for rec in aln:\n",
    "        seq = str(rec.seq)\n",
    "        for i in range(0, len(seq)):\n",
    "            if i + 1 not in occ:\n",
    "                occ[i+1] = 0\n",
    "            if seq[i] != '-':\n",
    "                occ[i+1] += 1\n",
    "    return occ\n",
    "\n",
    "\n",
    "def get_n_seq(seq_file, seq_format = \"fasta\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    seqs = Bio.SeqIO.parse(seq_file,seq_format)\n",
    "    n = 0\n",
    "    for seq in seqs:\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "def get_cons_cols(aln_in, fmt_in, t = 0.5): #returns the columns with a relative occupancy greater than a threshold\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_seq = get_n_seq(aln_in, fmt_in)\n",
    "    aln_occ = get_occ_cols(aln_in, fmt_in)\n",
    "    cons_cols = [col for col, occ in aln_occ.items() if occ/n_seq > t]\n",
    "    return cons_cols\n",
    "\n",
    "cons_cols = get_cons_cols(aln_in, aln_fmt, t = 0.50) # CONTAINS INDICES OF MSA COLUMNS WITH OCCUPANCY > 0.50 OR CONSENSUS\n",
    "print(len(cons_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_columns(aln_in, infmt):\n",
    "    aln = Bio.AlignIO.read(aln_in, infmt)\n",
    "    n_cols = len(aln[0])\n",
    "    cols = {}\n",
    "    for col in range(1,n_cols+1):\n",
    "        cols[col] = []\n",
    "    for row in aln:\n",
    "        seq = str(row.seq)\n",
    "        for i in range(0,len(seq)):\n",
    "            cols[i+1].append(seq[i])\n",
    "    return cols\n",
    "def get_freqs(col):\n",
    "    abs_freqs = {\n",
    "        'A':0, 'R':0, 'N':0, 'D':0,'C':0,'Q':0, 'E':0,'G':0,'H':0, 'I':0,\n",
    "        'L':0, 'K':0,'M':0, 'F':0, 'P':0,'S':0,'T':0, 'W':0, 'Y':0, 'V':0, '-':0\n",
    "    }\n",
    "    for aa in col:\n",
    "        aa = aa.upper()\n",
    "        if col.count('-') == len(col):\n",
    "            abs_freqs['-'] = 1\n",
    "            return abs_freqs\n",
    "        if aa not in ['-', 'X', 'B']:\n",
    "            abs_freqs[aa] += 1\n",
    "    rel_freqs = {k: v/(len(col)-(col.count('-')+col.count('X')+col.count('B'))) for k, v in abs_freqs.items()}\n",
    "    return rel_freqs\n",
    "def get_entropy(freqs):\n",
    "    S = 0\n",
    "    for f in freqs.values():\n",
    "        if f != 0:\n",
    "            S += f*math.log2(f)\n",
    "    return -S\n",
    "def get_shenkin(col):\n",
    "    S = get_entropy(get_freqs(col))\n",
    "    return (2**S)*6\n",
    "def get_stats(col):\n",
    "    n_seqs = len(col)\n",
    "    gaps = col.count('-')\n",
    "    occ = n_seqs - gaps\n",
    "    occ_pct = occ/n_seqs\n",
    "    gaps_pct = 1 - occ_pct\n",
    "    return occ, gaps, occ_pct, gaps_pct\n",
    "def calculate_shenkin(aln_in, infmt, t = 0.5):\n",
    "    cols = in_columns(aln_in, infmt)\n",
    "    scores = []\n",
    "    occ = []\n",
    "    gaps = []\n",
    "    occ_pct = []\n",
    "    gaps_pct = []\n",
    "    for k, v in cols.items():\n",
    "        scores.append(get_shenkin(v))\n",
    "        stats = (get_stats(v))\n",
    "        occ.append(stats[0])\n",
    "        gaps.append(stats[1])\n",
    "        occ_pct.append(stats[2])\n",
    "        gaps_pct.append(stats[3])\n",
    "    df = pd.DataFrame(list(zip(list(range(1,len(scores)+1)),scores, occ,gaps, occ_pct, gaps_pct)), columns = ['col','shenkin','occ','gaps','occ_pct','gaps_pct'])\n",
    "    #df = df[df.occ_pct >= t]\n",
    "    #max_shenkin = max(list(df.shenkin))\n",
    "    #df.index = range(1, len(df)+1)\n",
    "    return df\n",
    "shenkin_aln = calculate_shenkin(aln_in, aln_fmt)\n",
    "shenkin_aln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shenkin_aln_filt = shenkin_aln[shenkin_aln.occ_pct > 0.50]\n",
    "shenkin_aln_filt.index = range(1, len(shenkin_aln_filt)+1) # CONTAINS SHENKIN SCORE, OCCUPANCY/GAP PROPORTION OF CONSENSUS COLUMNS\n",
    "shenkin_aln_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_shenkin = min(shenkin_aln_filt.shenkin)\n",
    "\n",
    "shenkin_aln_filt.shenkin = shenkin_aln_filt.shenkin.astype(int)\n",
    "max_shenkin = np.max(shenkin_aln_filt.shenkin)\n",
    "shenkin_aln_filt[\"norm_shenkin_rel\"] = 100*(shenkin_aln_filt.shenkin-min_shenkin)/(max_shenkin-min_shenkin) # ADDING NEW COLUMNS WITH DIFFERENT NORMALISED SCORES\n",
    "shenkin_aln_filt[\"norm_shenkin_abs\"] = 100*(shenkin_aln_filt.shenkin-6)/(120-6)\n",
    "shenkin_aln_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conservation_mod(df, colors = [\"steelblue\", \"orange\", \"tomato\", \"orchid\"], cons_score_col = 'shenkin', cons_score_label = \"Shenkin score\", qs = [60,], out = None):\n",
    "\n",
    "\n",
    "\n",
    "    n_cols = len(df)\n",
    "    print(n_cols)\n",
    "\n",
    "    plt.figure(figsize=(180,80))\n",
    "    plt.rcParams.update({\"axes.linewidth\": 10})\n",
    "    palette = {}\n",
    "   \n",
    "    \n",
    "    \n",
    "    for i in df.index:\n",
    "        if df.loc[i, cons_score_col] <= qs[0]:\n",
    "            palette[i] = colors[0]\n",
    "        elif df.loc[i, cons_score_col] > qs[0] and df.loc[i, cons_score_col] <= qs[1]:\n",
    "            palette[i] = colors[1]\n",
    "        elif df.loc[i, cons_score_col] > qs[1] and df.loc[i, cons_score_col] <= qs[2]:\n",
    "            palette[i] = colors[2]\n",
    "        else:\n",
    "            palette[i] = colors[3]\n",
    "\n",
    "    \n",
    "            #r'$3_{10}$-helix'\n",
    "    legend_dict = { r'$N_{s} \\leq 25$' : colors[0], r'$25 < N_{s} \\leq 50$' : colors[1], r'$50 < N_{s} \\leq 75$' : colors[2], r'$N_{s} > 75$' : colors[3]}\n",
    "    patchList = []\n",
    "    for key in legend_dict:\n",
    "        data_key = mpatches.Patch(facecolor=legend_dict[key], label=key, edgecolor = \"black\", linewidth = 3)\n",
    "        patchList.append(data_key)\n",
    "\n",
    "    cols = ['SHENKIN', 'palettes']\n",
    "    lst = []\n",
    "    for x in df.index:\n",
    "        lst.append([df.norm_shenkin_rel  [x], palette[x]])\n",
    "    df1 = pd.DataFrame(lst, columns=cols)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ax=sns.barplot(df1, y=df1.SHENKIN, x=df1.index, linewidth = 0.01,  hue =df1.palettes, palette=[\"purple\", \"royalblue\", \"orange\", \"seagreen\"], edgecolor = 'black')\n",
    "    ax.tick_params(axis= 'both' , which = 'major', pad = 30, width = 15, length = 50, labelsize = 80)\n",
    "    ax.set_xlabel(\"Occupancy residue position\", labelpad = 50, fontsize = 120)\n",
    "    ax.set_ylabel(cons_score_label, labelpad = 50, fontsize = 120)\n",
    "    for i, q in enumerate(qs):\n",
    "        plt.axhline(y = q, color = \"black\", linewidth = 7.5, linestyle = '--') #colors[i]\n",
    "    plt.xlim(0,len(df))\n",
    "    plt.xticks(np.arange(0, n_cols, 20))\n",
    "    legend = plt.legend(handles=patchList, loc='center left', bbox_to_anchor=(1, 0.5),fontsize = 120)\n",
    "    legend.get_frame().set_linewidth(10)\n",
    "    legend.get_frame().set_edgecolor(\"black\")\n",
    "    for legobj in legend.legendHandles:\n",
    "        legobj.set_linewidth(7.5)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#\"steelblue\", \"forestgreen\", \"gold\", \"firebrick\"\n",
    "\n",
    "plot_conservation_mod(shenkin_aln_filt, colors = [\"purple\", \"seagreen\", \"royalblue\", \"orange\"],\n",
    "                                       cons_score_col = 'norm_shenkin_rel', cons_score_label = \"Normalised Shenkin divergence score\", qs = [25, 50, 75]) # PLOT NORMALISED SHENKIN DIVERGENCE SCORE PER POSITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISSENSE VARIANTS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "############################################################################################WRONG AFTER THIS \n",
    "\n",
    "\n",
    "def format_variant_table(df, col_mask, vep_mask = [\"missense_variant\"], tab_format = \"gnomad\"):\n",
    "    df_filt = df.copy(deep = True)\n",
    "    df_filt.reset_index(inplace=True)\n",
    "    if tab_format == \"gnomad\":\n",
    "        df_filt.columns = [' '.join(col).strip() for col in df_filt.columns.tolist()]\n",
    "    df_filt.columns = [col.lower().replace(\" \", \"_\") for col in df_filt.columns.tolist()]\n",
    "    df_filt = df_filt[df_filt.source_id.str.contains(\"HUMAN\")]\n",
    "    \n",
    "    df_filt = df_filt.dropna(subset=[\"vep_consequence\"])\n",
    "    df_filt = df_filt[df_filt.vep_consequence.isin(vep_mask) ]\n",
    "    df_filt = df_filt[df_filt.alignment_column.isin(col_mask)]\n",
    "    return df_filt\n",
    "human_missense_variants = format_variant_table(variant_table, cons_cols) # GET ONLY MISSENSE VARIANTS ROWS\n",
    "human_missense_variants\n",
    "\n",
    "\n",
    "def format_variant_table(df, col_mask, vep_mask = [\"synonymous_variant\"], tab_format = \"gnomad\"):\n",
    "    df_filt = df.copy(deep = True)\n",
    "    df_filt.reset_index(inplace=True)\n",
    "    if tab_format == \"gnomad\":\n",
    "        df_filt.columns = [' '.join(col).strip() for col in df_filt.columns.tolist()]\n",
    "    df_filt.columns = [col.lower().replace(\" \", \"_\") for col in df_filt.columns.tolist()]\n",
    "    df_filt = df_filt[df_filt.source_id.str.contains(\"HUMAN\")]\n",
    "    \n",
    "    df_filt = df_filt.dropna(subset=[\"vep_consequence\"])\n",
    "    df_filt = df_filt[df_filt.vep_consequence.isin(vep_mask) ]\n",
    "    df_filt = df_filt[df_filt.alignment_column.isin(col_mask)]\n",
    "    return df_filt\n",
    "human_synomous_variants = format_variant_table(variant_table, cons_cols) # GET ONLY human_synomous_variants VARIANTS ROWS\n",
    "human_synomous_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subset_aln(aln_in, aln_fmt, df, aln_out = None):\n",
    "    seqs_ids = df.source_id.unique().tolist()\n",
    "    aln = Bio.SeqIO.parse(aln_in, aln_fmt)\n",
    "    variant_seqs = [rec for rec in aln if rec.id in seqs_ids]\n",
    "    if aln_out == None:\n",
    "        pref, fmt = aln_in.split(\".\")\n",
    "        aln_out =  pref + \"_variant_seqs.\"+fmt\n",
    "    Bio.SeqIO.write(variant_seqs, aln_out, aln_fmt)\n",
    "    return aln_out\n",
    "\n",
    "def calculate_shenkin(aln_in, infmt, t = 0.5):\n",
    "    cols = in_columns(aln_in, infmt)\n",
    "    scores = []\n",
    "    occ = []\n",
    "    gaps = []\n",
    "    occ_pct = []\n",
    "    gaps_pct = []\n",
    "    for k, v in cols.items():\n",
    "        scores.append(get_shenkin(v))\n",
    "        stats = (get_stats(v))\n",
    "        occ.append(stats[0])\n",
    "        gaps.append(stats[1])\n",
    "        occ_pct.append(stats[2])\n",
    "        gaps_pct.append(stats[3])\n",
    "    df = pd.DataFrame(list(zip(list(range(1,len(scores)+1)),scores, occ,gaps, occ_pct, gaps_pct)), columns = ['col','shenkin','occ','gaps','occ_pct','gaps_pct'])\n",
    "    df = df[df.occ_pct >= t]\n",
    "    #max_shenkin = max(list(df.shenkin))\n",
    "    df.index = range(1, len(df)+1)\n",
    "    return df\n",
    "\n",
    "def get_OR(df, variant_col = \"variants\"):\n",
    "    tot_occ = sum(df.occ)\n",
    "    tot_vars = sum(df[variant_col])\n",
    "    idx = df.index.tolist()\n",
    "    for i in idx:\n",
    "        i_occ = df.loc[i,\"occ\"]\n",
    "        i_vars = df.loc[i,variant_col]\n",
    "        rest_occ = tot_occ - i_occ\n",
    "        rest_vars = tot_vars - i_vars\n",
    "        oddsr, pval = stats.fisher_exact([[i_vars, rest_vars], [i_occ, rest_occ]])\n",
    "        vals = [i_vars,rest_vars,i_occ, rest_occ]\n",
    "        se_logor = 1.96*(math.sqrt(sum(list(map((lambda x: 1/x),vals)))))\n",
    "        logor = math.log(oddsr)\n",
    "        df.loc[i,'oddsratio'] = oddsr\n",
    "        df.loc[i,'log_oddsratio'] = logor\n",
    "        df.loc[i,'pvalue'] = pval\n",
    "        df.loc[i,'ci_dist'] = se_logor\n",
    "    return df\n",
    "def get_missense_df(aln_in, aln_fmt, variants_df, cons_cols, shenkin_aln, aln_out = None, t = 0.5, get_or = True):\n",
    "    #variants_aln = generate_subset_aln(aln_in, aln_fmt, variants_df, aln_out)\n",
    "    variants_aln_info = calculate_shenkin(aln_in, aln_fmt, t)\n",
    "    variants_aln_info = variants_aln_info[variants_aln_info.occ_pct >= t]\n",
    "    cons_cols = get_cons_cols(aln_in, aln_fmt, t = t)\n",
    "    vars_df = pd.DataFrame(variants_df.alignment_column.value_counts().reindex(cons_cols, fill_value=0).sort_index()).reset_index()\n",
    "    \n",
    "    vars_df.index = range(1, len(vars_df)+1)\n",
    "    vars_df.columns = [\"col\", \"variants\"]\n",
    "    variants_aln_info = variants_aln_info[variants_aln_info.col.isin(cons_cols)]\n",
    "    \n",
    "    variants_aln_info.index = range(1, len(variants_aln_info)+1)\n",
    "    \n",
    "    merged = pd.merge(variants_aln_info, vars_df, on = \"col\", how = 'left')\n",
    "    \n",
    "    merged.index = range(1, len(merged)+1)\n",
    "    merged.variants = merged.variants + 1\n",
    "    merged[\"shenkin\"] = shenkin_aln[\"shenkin\"]\n",
    "    merged[\"norm_shenkin_rel\"] = shenkin_aln[\"norm_shenkin_rel\"] # REPLACES ONLY HUMAN DIVERGENCE SCORES BY ALL SPECIES, FULL ALIGNMENT PROVIDES A BETTER MEASURE FOR THE OVERALL DIVERGENCE\n",
    "    merged[\"norm_shenkin_abs\"] = shenkin_aln[\"norm_shenkin_abs\"]\n",
    "    \n",
    "    if get_or == True:\n",
    "        merged_or = get_OR(merged)\n",
    "        return merged_or\n",
    "    else:\n",
    "        return merged\n",
    "aln_out_human_missense_variants = \"/homes/2414054/varalign_folder/Altered_Alignment/Changing_Alignment/Human_Alignment127_PFAM.sto\"\n",
    "\n",
    "missense_variants_df = get_missense_df(\n",
    "    aln_in, aln_fmt, human_missense_variants, cons_cols,\n",
    "    shenkin_aln_filt, aln_out_human_missense_variants, t = 0.50\n",
    ") # CALCULATES ENRICHMENT IN MISSENSE VARIANTS AND ASSOCIATED P-VALUE AND 95% CI\n",
    "print(missense_variants_df)\n",
    "\n",
    "\n",
    "synomous_variants_df = get_missense_df(\n",
    "    aln_in, aln_fmt, human_synomous_variants, cons_cols,\n",
    "    shenkin_aln_filt, aln_out_human_missense_variants, t = 0.50\n",
    ") # CALCULATES ENRICHMENT IN MISSENSE VARIANTS AND ASSOCIATED P-VALUE AND 95% CI\n",
    "print(missense_variants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cons_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_miss_class(df, cons_col = \"shenkin\", thresholds = [30, 70], colours = [\"royalblue\", \"seagreen\", \"grey\", \"purple\", \"orange\"]):\n",
    "    for i in df.index:          \n",
    "        if df.loc[i, cons_col] <= thresholds[0] and df.loc[i, \"log_oddsratio\"] < 0:\n",
    "            df.loc[i,\"miss_class\"] = \"CMD\"\n",
    "        elif df.loc[i, cons_col] <= thresholds[0] and df.loc[i, \"log_oddsratio\"] > 0:\n",
    "            df.loc[i,\"miss_class\"] = \"CME\"\n",
    "        elif df.loc[i, cons_col] >= thresholds[1] and df.loc[i, \"log_oddsratio\"] < 0:\n",
    "            df.loc[i,\"miss_class\"] = \"UMD\"\n",
    "        elif df.loc[i, cons_col] >= thresholds[1] and df.loc[i, \"log_oddsratio\"] > 0:\n",
    "            df.loc[i,\"miss_class\"] = \"UME\"\n",
    "        else:\n",
    "            df.loc[i,\"miss_class\"] = \"None\"\n",
    "           \n",
    "    coloring = {\n",
    "        \"CMD\": colours[0],\n",
    "        \"CME\": colours[1],\n",
    "        \"UMD\": colours[3],\n",
    "        \"UME\": colours[4],\n",
    "        \"None\": colours[2]\n",
    "    }\n",
    "    df[\"miss_color\"] =  df.miss_class.map(coloring) \n",
    "    return df\n",
    "print(missense_variants_df)\n",
    "missense_variants_df = add_miss_class(missense_variants_df, cons_col = \"norm_shenkin_rel\", thresholds = [25,75]) # ADDS CLASSIFICATION ACCORDING TO MES AND NORMALISED SHENKIN\n",
    "missense_variants_df\n",
    "\n",
    "synomous_variants_df= add_miss_class(synomous_variants_df, cons_col = \"norm_shenkin_rel\", thresholds = [25,75]) # ADDS CLASSIFICATION ACCORDING TO MES AND NORMALISED SHENKIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shenkin_logOR(df, strat_column, color_column, cons_col = \"shenkin\", thresholds = [30,70], out = None, markers = [\"h\", \".\", \"s\", \"*\", \"^\"], colors = [\"darkred\", \"tan\", \"green\", \"darkblue\", \"yellow\"], label = True, error = True):\n",
    "    plt.figure(figsize=(140,100))\n",
    "    classes_df = sorted(df[strat_column].unique().tolist())\n",
    "    #print(classes_df)\n",
    "    i = 0\n",
    "    for c in classes_df:\n",
    "        #print(c)\n",
    "        df_c = df[df[strat_column] == c]\n",
    "        color = df_c[color_column].unique().tolist()[0]\n",
    "        plt.scatter(df_c[cons_col], df_c.log_oddsratio,  marker = markers[i], c = colors[i], label = c,  s = 5000  ) #, edgecolor = 'black', linewidth = 10)\n",
    "        if error == True:\n",
    "            plt.errorbar(df_c[cons_col],df_c.log_oddsratio, yerr=df_c.ci_dist, c = colors[i], linewidth = 1, linestyle=\"None\", capsize = 1.0, capthick = 1)\n",
    "        i += 1\n",
    "    if label == True:\n",
    "        for x, y, z in zip(df[cons_col], df.log_oddsratio, df.index):\n",
    "            label = \"{:d}\".format(z)\n",
    "            plt.annotate(label, # this is the text\n",
    "                            (x,y), # this is the point to label\n",
    "                            textcoords=\"offset points\", # how to position the text\n",
    "                            xytext=(-75,75), # distance from text to points (x,y)\n",
    "                            ha='right',\n",
    "                            fontsize = 60) # horizontal alignment can be left, right or center\n",
    "    #plt.title(\"Missense enrichment score relative to sequence divergence\", fontsize = 200, pad = 140)\n",
    "    plt.xlabel(\"Normalised Shenkin divergence score\", fontsize = 180, labelpad = 60)\n",
    "    plt.ylabel(\"MES\", fontsize = 180, labelpad = 60)\n",
    "    plt.tick_params(axis= 'both' , which = 'major', pad = 30, width = 15, length = 50, labelsize = 160)\n",
    "    #plt.yticks(np.arange(-0.6,0.3,0.1))\n",
    "    plt.xticks(np.arange(0,105,5))\n",
    "    plt.xlim(-5,105)\n",
    "    #plt.ylim(-1.5,1.25)\n",
    "    plt.axhline(color = \"black\", linewidth = 10, linestyle = '--')\n",
    "    plt.axvline(x= thresholds[0], color = \"black\", linewidth = 10, linestyle = '--')\n",
    "    plt.axvline(x= thresholds[1], color = \"black\", linewidth = 10, linestyle = '--')\n",
    "    legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize = 140)\n",
    "    legend.get_frame().set_linewidth(10)\n",
    "    legend.get_frame().set_edgecolor(\"black\")\n",
    "    for legobj in legend.legendHandles:\n",
    "        legobj.set_linewidth(7.5)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "significant_missense_variants_df = missense_variants_df.loc[missense_variants_df['pvalue'] < 0.05]\n",
    "print(len(significant_missense_variants_df))\n",
    "plot_shenkin_logOR(\n",
    "    significant_missense_variants_df, \"miss_class\", \"miss_color\",\n",
    "    markers = [\"D\", \"s\", \"o\", \"h\", \"^\"],\n",
    "    colors = [\"royalblue\", \"seagreen\", \"grey\", \"purple\", \"orange\"],\n",
    "    cons_col = \"norm_shenkin_rel\", thresholds = [25,75])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tot_occ_res(occ_dict, n_cols, contact_map = \"intra\", t = 0):\n",
    "    res_occ = {}\n",
    "    for i in list(occ_dict.keys()):\n",
    "        if i[0] not in res_occ:\n",
    "            res_occ[i[0]] = 0\n",
    "        if contact_map == \"intra\":\n",
    "            if abs(i[1] - i[0]) > t:\n",
    "                res_occ[i[0]] += occ_dict[(i[0],i[0])]\n",
    "    for x in range(1,n_cols,1):\n",
    "        Found = \"False\"\n",
    "        for y in res_occ.keys():\n",
    "            if x == y:\n",
    "                Found = \"True\"\n",
    "        if Found == \"False\":\n",
    "            res_occ[x] = 0\n",
    "    res_occ = dict(sorted(res_occ.items()))\n",
    "    return res_occ\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_tot_cons_res(cons_df, contact_map = \"intra\", t = 0):\n",
    "    res_cons = {}\n",
    "    idx = cons_df.index.tolist()\n",
    "    for i in idx:\n",
    "        if i not in res_cons:\n",
    "            res_cons[i] = 0\n",
    "        for j in idx:\n",
    "            if contact_map == \"intra\":\n",
    "                \n",
    "                if abs(j - i) > t:\n",
    "                    res_cons[i] += cons_df.loc[i,j]\n",
    "            elif contact_map == \"inter\":\n",
    "                if i == j:\n",
    "                    res_cons[i] += cons_df.loc[i,j]\n",
    "                else:\n",
    "                    res_cons[i] += (cons_df.loc[i,j] + cons_df.loc[j,i])         \n",
    "    return res_cons\n",
    "\n",
    "def get_OR(df):\n",
    "    df.contacts = df.contacts + 1 #pseudocount\n",
    "    tot_occ = sum(df.occ)\n",
    "    tot_cons = sum(df.contacts)\n",
    "    for i in df.index:\n",
    "        i_occ = df.loc[i,\"occ\"]\n",
    "        i_cons = df.loc[i,'contacts']\n",
    "\n",
    "        rest_occ = tot_occ - i_occ\n",
    "        rest_cons = tot_cons - i_cons\n",
    "        \n",
    "        oddsr, pval = stats.fisher_exact([[i_cons, rest_cons], [i_occ, rest_occ]])\n",
    "        vals = [i_cons,rest_cons,i_occ, rest_occ]\n",
    "        se_logor = 1.96*(math.sqrt(sum(list(map((lambda x: 1/x),vals)))))\n",
    "        logor = math.log(oddsr)\n",
    "        df.loc[i,'oddsratio'] = oddsr\n",
    "        df.loc[i,'log_oddsratio'] = logor\n",
    "        df.loc[i,'pvalue'] = pval\n",
    "        df.loc[i,'ci_dist'] = se_logor\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_OR_from_cons(res_cons, res_occ):\n",
    "    cons = list(res_cons.values())\n",
    "    occ = list(res_occ.values())\n",
    "    df = pd.DataFrame(list(zip(occ, cons)), columns =['occ', 'contacts'])\n",
    "    df.index = range(1,len(df)+1)\n",
    "    df = get_OR(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_colours(df, missense_variants_df ):\n",
    "    jioner = missense_variants_df[['miss_class','miss_color']].copy()\n",
    "    df2 = pd.merge(df, jioner, left_index=True, right_index=True)\n",
    "    return df2\n",
    "\n",
    "\n",
    "def fisher_exact_test(df,x):\n",
    "    \n",
    "    cmd_cons = df[df.miss_class == x].contacts.sum()\n",
    "    cmd_occ = df[df.miss_class == x].occ.sum()\n",
    "    rest_cons = df.contacts.sum() - cmd_cons\n",
    "    rest_occ = df.occ.sum() - cmd_occ\n",
    "    oddsr1, pval1 = stats.fisher_exact([[cmd_cons, rest_cons], [cmd_occ, rest_occ]]) # CMDs ARE NOT ENRICHED IN INTER-REPEAT CONTACTS\n",
    "\n",
    "    cmd_cons = df[df.miss_class == x].contacts.sum()\n",
    "    cmd_occ = df[df.miss_class == x].occ.sum()\n",
    "    rest_cons = df[df.miss_class != x].contacts.sum()\n",
    "    rest_occ = df[df.miss_class != x].occ.sum()\n",
    "    oddsr2, pval2 = stats.fisher_exact([[cmd_cons, rest_cons], [cmd_occ, rest_occ]]) # CMDs ARE ENRICHED IN INTRA-REPEAT CONTACTS RELATIV\n",
    "    if oddsr1 == oddsr2 and pval1 == pval2:\n",
    "        print(x, oddsr1, pval1)\n",
    "    else:\n",
    "        print(x, oddsr1, pval1)\n",
    "        print()\n",
    "        print(x, oddsr2, pval2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in (5,0):\n",
    "    intra_res_occ  = get_intra_cons_occ(df_rf, cons_cols_eq) # GETS COVERAGE OF EACH INTRA-REPEAT POSITION PAIR IN OUR STRUCTURAL DATASET\n",
    "    \n",
    "    occ_intra = get_tot_occ_res(intra_res_occ, len(intra), contact_map = \"intra\", t = y) # CALCULATES THE ABSOLUTE STRUCTURAL COVERAGE PER DOMAIN POSITION\n",
    "    cons_intra = get_tot_cons_res(intra, contact_map = \"intra\", t = y) # CALCULATES THE ABSOLUTE NUMBER OF CONTACTS PER DOMAIN POSITION\n",
    "    enrichment_df_intra = get_OR_from_cons(cons_intra, occ_intra) # CALCULATES OR, LOG(OR), P-VALUE AND 95% CI FOR THE ENRICHMENT IN CONTACTS\n",
    "    enrichment_df_intra = add_colours(enrichment_df_intra,missense_variants_df) # ADDS MISSENSE ENRICHMENT CLASS\n",
    "    for x in list(enrichment_df_intra.miss_class.unique()):\n",
    "        fisher_exact_test(enrichment_df_intra,x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment_df_intra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cons_enrichment(df, contact_map = \"Intra\", legend_title = None, class_col = None, color_col = None, out = None):\n",
    "    plt.figure(figsize=(140,60))\n",
    "    plt.rcParams.update({\"axes.linewidth\": 5})\n",
    "    #plt.title(\"Enrichment contacts per residue position\", fontsize = 140, pad = 100)\n",
    "    plt.xlabel(\"Occupancy residue position\", fontsize = 120, labelpad = 100)\n",
    "    plt.ylabel(\"Contacts enrichment score\".format(contact_map), fontsize = 120, labelpad = 80)\n",
    "    plt.tick_params(axis= 'both' , which = 'major', pad = 60, width = 5, length = 30, labelsize = 80)\n",
    "    df_len = len(df)+3\n",
    "    print(df_len)\n",
    "    plt.xticks(np.arange(0,df_len ,20))\n",
    "    plt.xlim(0,df_len)\n",
    "    plt.axhline(y = 0, color = \"black\", linewidth = 5)\n",
    "    lines_range = list(np.arange(0,df_len,5))\n",
    "    for point in lines_range:\n",
    "        plt.axvline(x= point, color = \"grey\", linestyle = '--', linewidth = 5)\n",
    "    if class_col != None and color_col != None:\n",
    "        classes = list(df[class_col].unique())\n",
    "        for c in classes:\n",
    "            df_c = df[df[class_col] == c]\n",
    "            color = df_c[color_col].unique().tolist()[0]\n",
    "            plt.scatter(df_c.index, df_c.log_oddsratio, c = color, label = c, s = 5000, edgecolor = 'black', linewidth = 10)\n",
    "            #plt.errorbar(df_c.index,df_c.log_oddsratio, yerr=df_c.ci_dist, c = color, linewidth = 7.5, linestyle=\"None\", capsize = 35.0, capthick = 7.5)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize = 80)#, title = legend_title, title_fontsize = 120)\n",
    "    else:\n",
    "        plt.scatter(df.index, df.log_oddsratio, s = 5000, edgecolor = 'black', linewidth = 7.5)\n",
    "        #plt.errorbar(df.index,df.log_oddsratio, yerr=df.ci_dist, linewidth = 7.5, linestyle=\"None\", capsize = 35.0, capthick = 7.5)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(len(enrichment_df_intra))\n",
    "plot_cons_enrichment(enrichment_df_intra, contact_map = \"Intra\", class_col = \"miss_class\", color_col = \"miss_color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cons_enrichment(df, contact_map = \"Intra\", legend_title = None, class_col = None, color_col = None, out = None):\n",
    "    plt.figure(figsize=(140,60))\n",
    "    plt.rcParams.update({\"axes.linewidth\": 5})\n",
    "    #plt.title(\"Enrichment contacts per residue position\", fontsize = 140, pad = 100)\n",
    "    plt.xlabel(\"Occupancy residue position\", fontsize = 120, labelpad = 100)\n",
    "    plt.ylabel(\"Contacts enrichment score\".format(contact_map), fontsize = 120, labelpad = 100)\n",
    "    plt.tick_params(axis= 'both' , which = 'major', pad = 60, width = 5, length = 30, labelsize = 80)\n",
    "    plt.xticks(np.arange(0,401,20))\n",
    "    plt.xlim(0,401)\n",
    "    plt.axhline(y = 0, color = \"black\", linewidth = 5)\n",
    "    lines_range = list(np.arange(0,401,5))\n",
    "    for point in lines_range:\n",
    "        plt.axvline(x= point, color = \"grey\", linestyle = '--', linewidth = 5)\n",
    "    if class_col != None and color_col != None:\n",
    "        classes = list(df[class_col].unique())\n",
    "        for c in classes:\n",
    "            df_c = df[df[class_col] == c]\n",
    "            color = df_c[color_col].unique().tolist()[0]\n",
    "            plt.scatter(df_c.index, df_c.log_oddsratio, c = color, label = c, s = 5000, edgecolor = 'black', linewidth = 10)\n",
    "            #plt.errorbar(df_c.index,df_c.log_oddsratio, yerr=df_c.ci_dist, c = color, linewidth = 7.5, linestyle=\"None\", capsize = 35.0, capthick = 7.5)\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize = 80)#, title = legend_title, title_fontsize = 120)\n",
    "    else:\n",
    "        plt.scatter(df.index, df.log_oddsratio, s = 5000, edgecolor = 'black', linewidth = 7.5)\n",
    "        plt.errorbar(df.index,df.log_oddsratio, yerr=df.ci_dist, linewidth = 7.5, linestyle=\"None\", capsize = 35.0, capthick = 7.5)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "significant_df = enrichment_df_intra.loc[enrichment_df_intra['pvalue'] < 0.05]\n",
    "plot_cons_enrichment(significant_df, contact_map = \"Intra\", class_col = \"miss_class\", color_col = \"miss_color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site = [97,98,99,283,284,285]\n",
    "for x in active_site:\n",
    "    print(x,enrichment_df_intra.miss_class[x], enrichment_df_intra.miss_color[x], enrichment_df_intra.pvalue[x], enrichment_df_intra.ci_dist[x])\n",
    "\n",
    "#print(x,significant_df.miss_class[284], significant_df.miss_color[284], significant_df.pvalue[284], significant_df.ci_dist[284])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site = [97,98,99,283,284,285]\n",
    "\n",
    "for x in active_site:\n",
    "    secondary = \"\"\n",
    "    \n",
    "    if ss_class_df.loc[x].p_helix > ss_class_df.loc[x].p_strand     and ss_class_df.loc[x].p_helix > ss_class_df.loc[x].p_coil      :\n",
    "        print(ss_class_df.loc[x].p_helix)\n",
    "        secondary = \"helix\"  \n",
    "    elif ss_class_df.loc[x].p_strand     > ss_class_df.loc[x].p_helix and ss_class_df.loc[x].p_strand     > ss_class_df.loc[x].p_coil      :    \n",
    "        print(ss_class_df.loc[x].p_strand)\n",
    "        secondary = \"strand\" \n",
    "    elif ss_class_df.loc[x].p_coil > ss_class_df.loc[x].p_helix and ss_class_df.loc[x].p_coil > ss_class_df.loc[x].p_strand    : \n",
    "        print(ss_class_df.loc[x].p_coil)   \n",
    "        secondary = \"loop\" \n",
    "    elif secondary == \"\":\n",
    "        if ss_class_df.loc[x].p_helix == ss_class_df.loc[x].p_strand:\n",
    "            secondary =  \"helix, strand\"\n",
    "            print(ss_class_df.loc[x].p_helix, ss_class_df.loc[x].p_strand)  \n",
    "        elif ss_class_df.loc[x].p_helix == ss_class_df.loc[x].p_coil:\n",
    "            secondary =  \"helix, loop\"\n",
    "            print(ss_class_df.loc[x].p_helix, ss_class_df.loc[x].p_coil)  \n",
    "        elif ss_class_df.loc[x].p_strand == ss_class_df.loc[x].p_coil:\n",
    "            secondary =  \"strand, loop\"    \n",
    "            print(ss_class_df.loc[x].p_strand, ss_class_df.loc[x].p_coil)  \n",
    "        \n",
    "        \n",
    "    print(secondary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Median RSA for active site\n",
    "active_site = [97,98,99,283,284,285]\n",
    "for x in active_site:\n",
    "    print(vs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site = [97,98,99,283,284,285]\n",
    "\n",
    "for x in active_site:\n",
    "    for y in range(1,len(intra_norm),1):\n",
    "        if intra_norm.loc[x,y] > 0.9:\n",
    "            print(x, y, intra_norm.loc[x,y])\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Active Site\n",
    "active_site = [97,98,99,283,284,285]\n",
    "for x in active_site:\n",
    "    print(shenkin_aln_filt.norm_shenkin_rel[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site = [97,98,99,283,284,285]\n",
    "\n",
    "for x in active_site:\n",
    "    secondary = \"\"\n",
    "    dictss = {}\n",
    "    dictss ={\"p_a_helix\": ss_df.loc[x].p_a_helix,\n",
    "    \"p_b_bridge\" : ss_df.loc[x].p_b_bridge ,\n",
    "    \"p_strand\" : ss_df.loc[x].p_strand,\n",
    "    \"p_3_10_helix\" : ss_df.loc[x].p_3_10_helix,\n",
    "    \"p_pi_helix\" : ss_df.loc[x].p_pi_helix,\n",
    "    \"p_turn\": ss_df.loc[x].p_turn,\n",
    "    \"p_bend\" : ss_df.loc[x].p_bend,\n",
    "    \"p_coil\" : ss_df.loc[x].p_coil}\n",
    "    #print(dictss)\n",
    "    maxim = 0\n",
    "    highest = \"\"\n",
    "    for y in dictss:\n",
    "        \n",
    "        if dictss[y] > maxim:\n",
    "            maxim = dictss[y]\n",
    "            secondary = y\n",
    "\n",
    "    print(secondary, maxim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aln_len(aln_in, fmt_in):\n",
    "    aln = Bio.AlignIO.read(aln_in, fmt_in)\n",
    "    return aln.get_alignment_length()\n",
    "\n",
    "def get_ppis(df, aln_len, col_mask):\n",
    "    df_filt = df.dropna(subset=['UniProt_dbAccessionId_A', 'UniProt_dbAccessionId_B'])\n",
    "    df_filt = df_filt[(df_filt.UniProt_dbAccessionId_A != df_filt.UniProt_dbAccessionId_B)&(df_filt.interaction_type == 'Protein-Protein')]\n",
    "    get_struc_info(df_filt)\n",
    "    df_reps = df_filt.groupby(\"SOURCE_ID_A\")\n",
    "    cons_norm = pd.DataFrame(0, index = range(1, aln_len+1), columns = [\"contacts\"])\n",
    "    for repeat, row in df_reps: # REPEAT\n",
    "        df_strucs = row.groupby(\"PDB_dbAccessionId_A\")\n",
    "        rep_cons = pd.DataFrame(0, index = range(1, aln_len+1), columns = [\"cons\"])\n",
    "        for struc, row in df_strucs: # STRUCTURE\n",
    "            df_chains = row.groupby(\"PDB_dbChainId_A\")\n",
    "            struc_cons = pd.DataFrame(0, index = range(1, aln_len+1), columns = [\"cons\"])\n",
    "            for chain, row in df_chains: # CHAIN\n",
    "                chain_cons = pd.DataFrame(row.groupby(\"Alignment_column_A\")[\"interaction_type\"].count()).reindex(range(1, aln_len+1)).fillna(0)\n",
    "                struc_cons.cons = struc_cons.cons + chain_cons.interaction_type\n",
    "            struc_cons.cons = struc_cons.cons.map(int).map(lambda x: 1 if x > 0 else 0)\n",
    "            rep_cons.cons = rep_cons.cons + struc_cons.cons\n",
    "        rep_cons.cons = rep_cons.cons.map(int).map(lambda x: 1 if x > 0 else 0)\n",
    "        cons_norm.contacts = cons_norm.contacts + rep_cons.cons\n",
    "    cons_norm = cons_norm[cons_norm.index.isin(col_mask)]\n",
    "    cons_norm.index = range(1,len(col_mask)+1)\n",
    "    return cons_norm\n",
    "\n",
    "\n",
    "\n",
    "def get_struc_res_occ(structure_table, col_mask, interaction_mask = None): #returns a dataframe with the occupancy of every alignment column in structure\n",
    "    if interaction_mask != None:\n",
    "        structure_table_interaction = structure_table[(structure_table.UniProt_dbAccessionId_A != structure_table.UniProt_dbAccessionId_B)&(structure_table.interaction_type == interaction_mask)]\n",
    "        structures_mask = structure_table_interaction.PDB_dbAccessionId_A.unique().tolist()\n",
    "        structure_table = structure_table[structure_table.PDB_dbAccessionId_A.isin(structures_mask)]\n",
    "    table_a = structure_table[['UniProt_dbAccessionId_A','UniProt_dbResNum_A','Alignment_column_A']]\n",
    "    table_a = table_a.drop_duplicates(['UniProt_dbAccessionId_A','UniProt_dbResNum_A']).dropna()\n",
    "    table_a.Alignment_column_A = table_a.Alignment_column_A.astype(float).astype(int)\n",
    "    table_a = table_a.rename(columns={'UniProt_dbAccessionId_A':'UniProt_dbAccessionId','UniProt_dbResNum_A':'UniProt_dbResNum','Alignment_column_A':'Alignment_column'})\n",
    "    \n",
    "    table_b = structure_table[['UniProt_dbAccessionId_B','UniProt_dbResNum_B','Alignment_column_B']]\n",
    "    table_b = table_b.drop_duplicates(['UniProt_dbAccessionId_B','UniProt_dbResNum_B']).dropna()\n",
    "    table_b.Alignment_column_B = table_b.Alignment_column_B.astype(float).astype(int)\n",
    "    table_b = table_b.rename(columns={'UniProt_dbAccessionId_B':'UniProt_dbAccessionId','UniProt_dbResNum_B':'UniProt_dbResNum','Alignment_column_B':'Alignment_column'})\n",
    "\n",
    "    table_ab = pd.concat([table_a,table_b])\n",
    "    table_ab = table_ab.drop_duplicates(['UniProt_dbAccessionId','UniProt_dbResNum'])\n",
    "    res_occ = pd.DataFrame(table_ab.Alignment_column.value_counts())\n",
    "    res_occ = res_occ[res_occ.index.isin(col_mask)]\n",
    "    \n",
    "    \n",
    "    res_occ.reset_index(inplace = True)\n",
    "    res_occ = res_occ.rename(columns={'Alignment_column': 'occ'})\n",
    "    res_occ.index.name = 'aln_col'\n",
    "    print(res_occ)\n",
    "    print(col_mask)\n",
    "    #res_occ = res_occ[res_occ.index.isin(col_mask)]\n",
    "    res_occ = res_occ.sort_values(by=['aln_col'])#\n",
    "    \n",
    "    res_occ.index = range(1,len(res_occ)+1)\n",
    "    return res_occ\n",
    "\n",
    "\n",
    "\n",
    "def get_OR(df):\n",
    "    df.contacts = df.contacts + 1 #pseudocount\n",
    "    tot_occ = sum(df.occ)\n",
    "    tot_cons = sum(df.contacts)\n",
    "    for i in df.index:\n",
    "        i_occ = df.loc[i,\"occ\"]\n",
    "        i_cons = df.loc[i,'contacts']\n",
    "        rest_occ = tot_occ - i_occ\n",
    "        rest_cons = tot_cons - i_cons\n",
    "        oddsr, pval = stats.fisher_exact([[i_cons, rest_cons], [i_occ, rest_occ]])\n",
    "        vals = [i_cons,rest_cons,i_occ, rest_occ]\n",
    "        se_logor = 1.96*(math.sqrt(sum(list(map((lambda x: 1/x),vals)))))\n",
    "        logor = math.log(oddsr)\n",
    "        df.loc[i,'oddsratio'] = oddsr\n",
    "        df.loc[i,'log_oddsratio'] = logor\n",
    "        df.loc[i,'pvalue'] = pval\n",
    "        df.loc[i,'ci_dist'] = se_logor\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_struc_res_occ(structure_table, col_mask, interaction_mask = None): #returns a dataframe with the occupancy of every alignment column in structure\n",
    "    print(interaction_mask)\n",
    "    if interaction_mask != None:\n",
    "        structure_table_interaction = structure_table[(structure_table.UniProt_dbAccessionId_A != structure_table.UniProt_dbAccessionId_B)&(structure_table.interaction_type == interaction_mask)]\n",
    "        structures_mask = structure_table_interaction.PDB_dbAccessionId_A.unique().tolist()\n",
    "        structure_table = structure_table[structure_table.PDB_dbAccessionId_A.isin(structures_mask)]\n",
    "    table_a = structure_table[['UniProt_dbAccessionId_A','UniProt_dbResNum_A','Alignment_column_A']]\n",
    "    table_a = table_a.drop_duplicates(['UniProt_dbAccessionId_A','UniProt_dbResNum_A']).dropna()\n",
    "    table_a.Alignment_column_A = table_a.Alignment_column_A.astype(float).astype(int)\n",
    "    table_a = table_a.rename(columns={'UniProt_dbAccessionId_A':'UniProt_dbAccessionId','UniProt_dbResNum_A':'UniProt_dbResNum','Alignment_column_A':'Alignment_column'})\n",
    "    \n",
    "    table_b = structure_table[['UniProt_dbAccessionId_B','UniProt_dbResNum_B','Alignment_column_B']]\n",
    "    table_b = table_b.drop_duplicates(['UniProt_dbAccessionId_B','UniProt_dbResNum_B']).dropna()\n",
    "    table_b.Alignment_column_B = table_b.Alignment_column_B.astype(float).astype(int)\n",
    "    table_b = table_b.rename(columns={'UniProt_dbAccessionId_B':'UniProt_dbAccessionId','UniProt_dbResNum_B':'UniProt_dbResNum','Alignment_column_B':'Alignment_column'})\n",
    "\n",
    "    table_ab = pd.concat([table_a,table_b])\n",
    "    table_ab = table_ab.drop_duplicates(['UniProt_dbAccessionId','UniProt_dbResNum'])\n",
    "    res_occ = pd.DataFrame(table_ab.Alignment_column.value_counts())\n",
    "    res_occ.reset_index(inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    res_occ = res_occ.rename(columns={'index':'aln_col','Alignment_column':'occ'})\n",
    "    \n",
    "    res_occ.sort_index(inplace=True)\n",
    "    col_mask = range(1,len(col_mask)+1, 1)\n",
    "    res_occ = res_occ[res_occ.index.isin(col_mask)]\n",
    "\n",
    "    \n",
    "\n",
    "    return res_occ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aln_len_allsp = get_aln_len('/homes/2414054/varalign_folder/Altered_Alignment/Changing_Alignment/Alignment127_PFAM.sto', 'stockholm') # ALIGNMENT LENGTH (NUMBER OF COLUMNS)\n",
    "cons_norm_allsp = get_ppis(df_allsp_rsrz_filt, aln_len_allsp, cons_cols_allsp) # PPI CONTACTS PER POSITION\n",
    "\n",
    "\n",
    "res_occ_allsp = get_struc_res_occ(df_allsp, cons_cols_allsp, \"Protein-Protein\") # STRUCTURAL COVERAGE OF EACH POSITION IN OUR DATASET\n",
    "contacts_allsp_Protein = pd.concat([res_occ_allsp, cons_norm_allsp], axis = 1)\n",
    "contacts_allsp_Protein = get_OR(contacts_allsp_Protein) # CALCULATES ENRICHMENT IN PPIs AS WELL AS P-VALUE AND 95% CI\n",
    "contacts_allsp_Protein = add_colours(contacts_allsp_Protein,missense_variants_df) # ADDS MISSENSE ENRICHMENT CLASS\n",
    "contacts_allsp_Protein[contacts_allsp_Protein['contacts'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_colours(df, missense_variants_df ):\n",
    "    jioner = missense_variants_df[['miss_class','miss_color']].copy()\n",
    "    df2 = pd.merge(df, jioner, left_index=True, right_index=True)\n",
    "    return df2\n",
    "\n",
    "def plot_ppi_enrichment(df, Yaxis_name, legend_title = None, class_col = None, color_col = None, out = None, significant=False, error=True):\n",
    "    if significant == True:\n",
    "        df = df.loc[df['pvalue'] < 0.05]\n",
    "\n",
    "    plt.figure(figsize=(180,80))\n",
    "    #plt.title(\"Enrichment in PPIs per residue position\", fontsize = 140, pad = 100)\n",
    "    plt.xlabel(\"Domain position\", fontsize = 160, labelpad = 100)\n",
    "    plt.ylabel(Yaxis_name, fontsize = 160, labelpad = 100)\n",
    "    plt.tick_params(axis= 'both' , which = 'major', pad = 60, width = 15, length = 50, labelsize = 140)\n",
    "    plt.xticks(np.arange(1,len(df),20))\n",
    "    plt.xlim(0,len(df))\n",
    "    plt.axhline(y = 0, color = \"black\", linewidth = 7.5)\n",
    "    lines_range = list(np.arange(5,len(df),5))\n",
    "    for point in lines_range:\n",
    "        plt.axvline(x= point, color = \"grey\", linestyle = '--', linewidth = 7.5)\n",
    "    plt.xticks(np.arange(0,len(df),20))\n",
    "    if class_col != None and color_col != None:\n",
    "        classes = list(df[class_col].unique())\n",
    "        for c in classes:\n",
    "            df_c = df[df[class_col] == c]\n",
    "            color = df_c[color_col].unique().tolist()[0]\n",
    "            plt.scatter(df_c.index, df_c.log_oddsratio, c = color, label = c, s = 10000, edgecolor = 'black', linewidth = 10)\n",
    "            if error == True:\n",
    "                plt.errorbar(df_c.index,df_c.log_oddsratio, yerr=df_c.ci_dist, c = color, linewidth = 10, linestyle=\"None\", capsize = 35.0, capthick = 7.5)\n",
    "        legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize = 120)#, title = legend_title, title_fontsize = 120)\n",
    "        legend.get_frame().set_linewidth(10)\n",
    "        legend.get_frame().set_edgecolor(\"black\")\n",
    "        for legobj in legend.legendHandles:\n",
    "            legobj.set_linewidth(7.5)\n",
    "    else:\n",
    "        plt.scatter(df.index, df.log_oddsratio, s = 7500, edgecolor = 'black', linewidth = 10)\n",
    "        plt.errorbar(df.index,df.log_oddsratio, yerr=df.ci_dist, linewidth = 10, linestyle=\"None\", capsize = 35.0, capthick = 7.5)\n",
    "    if out != None:\n",
    "        plt.savefig(out)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_ppi_enrichment(contacts_allsp_Protein,Yaxis_name=\"PPIES\",legend_title= \"Surface\", class_col =  \"miss_class\", color_col= \"miss_color\", significant=False, error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_occ_allsp = get_struc_res_occ(df_allsp, cons_cols_allsp, \"Protein-Ligand\") # STRUCTURAL COVERAGE OF EACH POSITION IN OUR DATASET\n",
    "contacts_allsp_Ligand = pd.concat([res_occ_allsp, cons_norm_allsp], axis = 1)\n",
    "contacts_allsp_Ligand = get_OR(contacts_allsp_Ligand) # CALCULATES ENRICHMENT IN PPIs AS WELL AS P-VALUE AND 95% CI\n",
    "contacts_allsp_Ligand = add_colours(contacts_allsp_Ligand,missense_variants_df) # ADDS MISSENSE ENRICHMENT CLASS\n",
    "plot_ppi_enrichment(contacts_allsp_Ligand,Yaxis_name=\"PPIES, Ligand\",legend_title= \"Surface\", class_col =  \"miss_class\", color_col= \"miss_color\", significant=False, error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts_allsp_Ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_exact_test_prot_lig(df,typess =\"Protein-Protein\"):\n",
    "    \n",
    "    for x in list(df.miss_class.unique()):\n",
    "        ppis_cons = df[df.miss_class == x].contacts.sum()\n",
    "        ppis_occ = df[df.miss_class == x].occ.sum()\n",
    "        rest_cons = df.contacts.sum() - ppis_cons\n",
    "        rest_occ = df.occ.sum() - ppis_occ\n",
    "        \n",
    "        oddsratio, pvalue = stats.fisher_exact([[ppis_cons, rest_cons], [ppis_occ, rest_occ]]) # BURIED RESIDUES DEPLETED IN PPIS RELATIVE TO SURFACE RESIDUES\n",
    "        #print(x, math.log(oddsratio), pvalue)\n",
    "        print(x,typess, oddsratio, pvalue)\n",
    "\n",
    "fisher_exact_test_prot_lig(contacts_allsp_Protein, typess =\"Protein-Protein\")\n",
    "fisher_exact_test_prot_lig(contacts_allsp_Ligand, typess =\"Protein-Ligand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = synomous_variants_df['norm_shenkin_rel']\n",
    "y = synomous_variants_df['variants']\n",
    "\n",
    "plt.figure(figsize=(180,80))\n",
    "\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "plt.scatter(x, y,s = 500, edgecolor = 'black', linewidth = 1)\n",
    "plt.plot(x, a*x+b) \n",
    "plt.xlabel('norm_shenkin_rel')\n",
    "plt.ylabel('variants')\n",
    "plt.xlabel(\"norm_shenkin_rel\", fontsize = 160, labelpad = 100)\n",
    "plt.ylabel(\"variants\", fontsize = 160, labelpad = 100)\n",
    "plt.tick_params(axis= 'both' , which = 'major', pad = 60, width = 15, length = 50, labelsize = 140)\n",
    "plt.xticks(np.arange(0,100,10))\n",
    "plt.xlim(0,100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missense_variants_df['norm_shenkin_rel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missense_variants_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3-ANK_analysis]",
   "language": "python",
   "name": "conda-env-miniforge3-ANK_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
